[[configuration]]
== Configuration

include::common/configuration-gui.adoc[]

[[settings]]
=== Settings

The options under the *Settings* area provide a hierarchical view of options that allow you to configure global options for the infrastructure of your {product-title} environment. At the top level, you have *Settings* including users, LDAP Groups, account roles, capacity and utilization collection, tag categories, values, and imports, custom variable imports, and license uploads. When you click on *Settings* and expand it, you can configure *Analysis Profiles*, *Zones*, and *Schedules*.

image:2041.png[]

When you go the *Settings* accordion, you are automatically taken to the server list under *Zones*.

[[regions]]
==== Regions

Use *Regions* for centralizing data which is collected from public and private virtualization environments. A region is ultimately represented as a single database for the VMDB. Regions are particularly useful when multiple geographical locations need to be managed as they enable all the data collection to happen at each particular location and avoid data collection traffic across slow links between networks.

When multiple regions are being used, each with their own unique ID, a master region can be created to centralize the data of all the children regions into a single master database. To do this, configure each child region to replicate its data to the master region database (Red Hat recommends use of region 99). This parent and child region is a one-to-many relationship.

Regions can contain multiple zones, which in turn contain appliances. Zones are used for further segregating network traffic along with enabling failover configurations. Each appliance has the capability to be configured for a number of specialized server roles. These roles are limited to the zone containing the appliance they run on.

Only one failover type of each server role can run in a zone. If multiple appliances have the same failover role, the extras are used as backups that activate only if the primary appliance fails. Non-failover server roles can run on multiple appliances simultaneously in a zone, so resources can be adjusted according to the workload those roles are responsible for.

The following diagram demonstrates an example of the multiple regions working together in a {product-title} environment.

image:7151.png[]

The Master appliance is located in Chicago and contains a master region and a subregion that manages the worker appliances. The Mahwah technology center contains a single subregion that manages two zones. Likewise the San Diego technology center contains a single subregion managing a single zone.

[NOTE]
======
* Replicating a parent region to a higher-level parent is not supported.
* Parent regions can be configured after the child regions are online.
======

The following diagram provides a closer look at a region:
image:7150.png[]

In this region, we have several {product-title} appliances acting as UI nodes and worker nodes. These worker nodes execute tasks on the providers in your environment. The Region also uses a region database that reports to a master database on the main {product-title} appliance. All appliances can connect to the authentication services (Active Directory, LDAP, Identity Management), outgoing mail (SMTP), and network services (SNMP).

[[region-scope]]
===== Region Scope

Regions are used to consolidate data from multiple VMDBs to a central database. The database at the top level, the master VMDB, cannot be used for operational tasks such as SmartState Analysis or Capacity and Utilization data collection. It is intended for use as a reporting database that includes all information across multiple subordinate regions. The subordinate regions replicate their information to the master.

[NOTE]
======
The subordinate regions are not aware of each other from a database perspective. You cannot see information from one subordinate region in another. The only VMDB with data visibility to all subordinate regions is the top level.
======

*Master Regions Scope*

* Reports all information from all subordinate VMDBs reporting up to it.
* Can perform power operations on virtual machines from subordinate regions.
* Controls its own access control list.

*Subordinate Regions Scope*

* Each subordinate controls its own access control independent of the other regions.
* Can only do work (such as SmartState Analysis and Capacity and Utilization collection) in its own region.
* Has no knowledge of the other regions.
* Replicates its data up to the master region.

[[region-settings]]
===== Region Settings

In the *Region* area, set items that apply to your entire {product-title} infrastructure such as users, LDAP Groups, capacity and utilization collection, company tags and tag categories, and licensing.
Regions are also used for database replication.

[[capacity-and-utilization-collections]]
===== Capacity and Utilization Collections

[[capacity-and-utilization-collection-settings]]
====== Capacity and Utilization Collection Settings

Use *C & U Collection Settings* to select specifically which clusters and datastores you want to collect usage data for. By selecting a cluster, you are choosing to collect data for all hosts and virtual machines that are part of that cluster. You must also have a server with the Capacity & Utilization *Coordinator*, *Data Collector*, and *Data Processor* roles enabled as well. See Section *Server Control Settings*.

After a provider has been discovered and its relationships refreshed, view the clusters, hosts, and datastores from the settings menu. Navigate to *Configuration*, then click on the menu:Settings[Region > C & U Collection] tab.

[[enabling-a-cluster,-host,-or-datastore-for-capacity-and-utilization-collection]]
====== Enabling a Cluster, Host, or Datastore for Capacity and Utilization Collection

To enable a cluster, host, or datastore for Capacity and Utilization Collection:

. From the settings menu, select *Configuration*, then click on the *Settings* accordion.
. Select *Region*, then click on the *C & U Collection* tab.
. In the *Clusters* area, check all clusters and hosts that you want to collect data for.
. In the *Datastores* area, check all datastores that you want to collect data for.
. Click *Save*.


[NOTE]
=========================
. As new clusters, hosts, and datastores are discovered, you will need to come back to this configuration to enable collection of capacity and utilization data unless you have used the *Collect for All* check boxes.
. *Collect for All Clusters* must be checked to be able to collect capacity and utilization data from cloud providers such as Red Hat OpenStack Platform or Amazon EC2.
=========================

[[tags]]
===== Tags

[[company-tag-categories-and-tags]]
====== Company Tag Categories and Tags

{product-title} allows you to create your own set of tags and tag categories. Use tags to create a customized, searchable index for your resources. Depending on your database type, your tags may be case sensitive. After creating these values, you can apply them to your resources. There are two kinds of tags.

* *Company tags* which you will see under *My Company Tags* for a resource. Create company tags from the settings menu. Navigate to *Configuration*, then click the *Settings* accordion, then menu:Region[My Company Tags]. A selection of company tags is provided to you by default as samples. These can be deleted if you do not need them, but are not recreated by {product-title}.

* *System tags* are assigned automatically by {product-title}.


[[creating-a-tag-category]]
====== Creating a Tag Category

To create a tag category:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then *Region*, then click on the *My Company Categories* tab.
. Click *Add* under the *Actions* column to create a new category.
. In the *Category Information* area:
image:2042.png[]
* Use *Name* to create a short name that refers to category in the VMDB.
+
[NOTE]
======
The *Name* and *Single Value* fields cannot be changed after the category has been added.
======
+
* Use *Description* to type a brief explanation of how the category should be used. This shows when you try to add a value to the category.
* Use *Long Description* to type a detailed explanation of the category.
* Set *Show in Console* to `ON` when the category is ready for use in the console. For example, you want to populate values for the category before exposing it to users.
* Set *Single Value* to `ON` for categories that can only have a single value assigned to a resource. For example, a virtual machine can only be assigned to one location, but could belong to more than one department. This cannot be changed after the category is created.
* Set *Capture C & U Data by Tag* to `ON` for the ability to group capacity and utilization data by this tag category. To use this, be sure to assign this tag to all the resources that you want to group by.
. Click *Add*.

Repeat these steps for each category you need. After you have created the category, you can add values to it.


[IMPORTANT]
======
If no values are created for a category, you are unable to assign a value from that category nor be able to filter by that category.
======

[[deleting-a-tag-category]]
====== Deleting a Tag Category

To delete a tag category:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then *Region*, then click on the *My Company Categories* tab.
. Click *Delete* under the *Actions* column for the category you want to delete.
. Click *OK* to confirm.

[NOTE]
======
When you delete a tag category, the category values are removed, and any tags from the category are unassigned from all resources.
======

[[creating-a-company-tag]]
====== Creating a Company Tag

To create a company tag:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then *Region*, then click on the *My Company Tags* tab.
. In the *Choose a Category* area, select a category from the *Category* list.
+
[NOTE]
======
* Some categories only allow one value to be assigned to a resource.
* For some databases such as *PostgreSQL*, tags are case sensitive. For example, filtering by 'Linux' in title case give you different results from filtering by 'linux' in lower case.
======
+
. Click *Add* under the *Actions* column, and type a *Name* and *Description* for your new value.
. Click *Add* once again to add the new entry to the table.

[[deleting-a-company-tag]]
====== Deleting a Company Tag

To delete a company tag:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then *Region*, then click on the *My Company Tags* tab.
. Click *Delete* under the *Actions* column next to the tag to delete it.
+
[NOTE]
======
When you delete a tag, the tag is also deleted from any resource to which it was assigned.
======
+
. Click *OK* to confirm.


[[importing-tags-for-virtual-machines]]
====== Importing Tags for Virtual Machines

You can import a *CSV* file with tag assignments into the VMDB. For the import to be successful, be aware of the following:

* The file must be in the following format, with one line for each virtual machine. One virtual machine per tag must be on a separate line even if you are assigning multiple tags of the same category.
* You must use the display names of the category and the display name for the tag for the import to work.
+
------
name,category,entry
evm2,Provisioning Scope,All
evm2,Exclusions,Do not Analyze
evm2,EVM Operations,Analysis Successful
rhel6,Department,Presales
rhel6,Department,Support
------
+


[[importing-tags-for-a-virtual-machine-from-a-csv-file]]
====== Importing Tags for a Virtual Machine from a CSV File

To import tags for a virtual machine from a CSV file:

. Make sure the *CSV file* is in the required format.
. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then *Region*, then click on the *Import Tags* tab.
. Click *Choose file* to go to the location where the file is located.
. Click *Upload*.
+
[NOTE]
======
If there are any problems with the file, such as an incorrect column name, unknown virtual machine, unknown tag, or multiple values for a tag that should have only one, an error message will appear in the console for those records.
======
+
. Click *Apply*.

[[importing-custom-values-for-virtual-machines-and-hosts]]
====== Importing Custom Values for Virtual Machines and Hosts

You can import a *CSV* file with asset tag information into the VMDB for a virtual machine or import custom values for hosts.
For the import to be successful, the file must be in the following format, with one line for each virtual machine or host.

* There are two columns.
* The first line of the file must have the column names as shown below.
* The column names are case sensitive.
* Each value must be separated by a comma.

*Virtual Machine Import Example*

------
name,custom_1
Ecommerce,665432
Customer,883452
SQLSrvr,1090430
Firewall,8230500
------

For virtual machines, the value for custom_1 will show in the *VM Summary* page as the *Custom Identifier* page as the *Custom Identifier* in the *Properties* area. All of the custom values will show in the *Custom Fields* area.

*Host Import Example*

------
hostname,custom_1,custom_2
esx303.galaxy.local,15557814,19948399
esxd1.galaxy.local,10885574,16416993
esxd2.galaxy.local,16199125,16569419
------

For hosts, the value for custom_1 will show in the *Host Summary* page as the *Custom Identifier* in the *Properties* area. All of the custom values will show in the *Custom Fields* area.

[[importing-asset-tags-for-a-virtual-machine-from-a-csv-file]]
====== Importing Asset Tags for a Virtual Machine from a CSV File

To import asset tags for a virtual machine from a CSV file

. Make sure the *CSV file* is in the required format.
. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then *Region*, then click on the *Import* tab.
. Select the type of custom variable you want to import, either *Host* or *VM*.
. Click *Choose file* to go to the location where the custom variable file is located.
. Click *Upload*.
+
[NOTE]
======
If there are any problems with the file, such as an incorrect column name, unknown virtual machine or host, a message appears.
======
+
. Click *Apply*.

ifdef::cfme[]
[[registering-and-updating]]
===== Registering {product-title}

You can register appliances, edit customer information, and apply {product-title_short} updates from the *Red Hat Updates* tab, accessible from the settings menu, and navigating to menu:Configuration[Region] in the user interface. You can register your appliance to either Red Hat Content Delivery Network (CDN) or to a Red Hat Satellite server, which assign the necessary update packages to the {product-title} server. The subscription management service you register with will provide your systems with updates and allow additional management.

The following tools are used during the update process:

* `Yum` provides package installation, updates, and dependency checking.
* `Red Hat Subscription Manager` manages subscriptions and entitlements.
* `Red Hat Satellite Server` provides local system registration and updates from inside the customer’s firewall.

[IMPORTANT]
======
The update worker synchronizes the VMDB with the status of available {product-title} content every 12 hours.
======

[NOTE]
======
Servers with the `RHN Mirror` role also act as a repository for other appliances to pull {product-title} package updates.
======

[[registering-appliances]]
====== Registering Appliances

include::common/configuration-register-appliance.adoc[]

[NOTE]
====
To update your appliances, see https://access.redhat.com/documentation/en-us/red_hat_cloudforms/4.6-beta/html-single/migrating_to_red_hat_cloudforms_4.6-beta/[Updating {product-title}] in _Migrating to {product-title} 4.6-beta_.
====

[[subscription-management-for-virtual-environments]]
====== Subscription Management for Virtual Environments

Customers can license {product-title} for a limited set of providers. This ability is enabled by providing entitlement certificates that describe the features to be enabled. {product-title} can be shipped as a bundled product with other Red Hat products like Red Hat OpenStack Platform and Red Hat OpenShift, providing advanced management capabilities to these products.

Entitlements provides the following enhancements:

* Ability to enable or disable providers based upon a certificate.
* Active subscription with Red Hat Cloud Data Network for delivery to {product-title}.
* Ability to remain in its own {product-title} channel.
* Ability to add providers even if no certificate is found.
* In the presence of a certificate, providers are limited as per SKU, the certificate is supporting.
* Ability to support the provider to SKU mapping.
* Providers remain fully functional even after adding or removing SKU associated with certificates.

endif::cfme[]

[[help-menu]]
===== Customizing the Help Menu

{product-title} allows administrators to customize the help menu. Use this feature to define menu labels, URLs and how each window opens for users. 

[NOTE]
====
Any change to the help menu will take effect upon a full page reload.
====

Customize the help menu using the following steps:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then *Region*.
. Click on the *Help Menu* tab.
. Provide custom *Menu item labels* and an associated *URL* for each. Define how each window should open by selecting from the options in the *Open in* menu. 
. Click *Submit*. 

[[profiles]]
==== Profiles

[[creating-an-analysis-profile]]
===== Creating an Analysis Profile

You can create an analysis profile by referring to the sample profiles provided in the console. You can copy the sample profile or create a new one.

[[creating-a-host-analysis-profile]]
===== Creating a Host Analysis Profile

To create a host analysis profile:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Analysis Profiles*.
. Click image:1847.png[] (*Configuration*), and image:plus_green.png[] (*Add Host Analysis Profile*).
. In the *Basic Information* area, type in a *Name* and *Description* for the analysis profile.
image:2048.png[]
. Click *File* to collect information about a file or group of files.
. From the *File Entry* area, click image:plus_green.png[] (*Click to add a new entry*) to add a file or group of files.
image:2047.png[]
* Check *Collect Contents* to not only check for existence, but also gather the contents of the file. If you do this, then you can use the contents to create policies in {product-title} Control.
. Click *Event Log* to specify event log entries to collect.
. From the *Event Log Entry* area, click image:plus_green.png[] (*Click to add a new entry*) to add a type of event log entry. Type in a *Name*. You can type in a specific message to find in *Filter Message*. In *Level*, set the value for the level of the entry and above. Specify the *Source* for the entry.
Finally, set the # number of days that you want to collect event log entries for. If you set this to 0, it will go as far back as there is data available.
image:2046.png[]
. Click *Add*.

[[creating-a-virtual-machine-analysis-profile]]
===== Creating a Virtual Machine Analysis Profile

To create a virtual machine analysis profile:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Analysis Profiles*.
. Click image:1847.png[] (*Configuration*), and image:plus_green.png[] (*Add VM Analysis Profile*).
. In the *Basic Information* area, type in a *Name* and *Description* for the analysis profile.
image:2050.png[]
. You begin in the *Category* tab. From the *Category Selection* area, check the categories you want to collect information for. This is available for virtual machine profiles only.
image:2051.png[]
. Click the *File* tab to collect information about a file or group of files.
. From the *File Entry* area, click image:plus_green.png[] (*Add this entry*) to add a file or group of files, then type a name. For virtual machines, specify the file to check for. Check the box under *Collect Contents* if you want to collect the file contents as well.
The files can be no larger than 1 MB.
image:2052.png[]
. Click the *Registry* tab to collect information on a registry key.
. From the *Registry Entry* area, type your *Registry Key* and *Registry Value*. To evaluate whether a registry key exists or does not exist on a virtual machine, without providing a value, type * in the *Registry Value* field.
Then, you do not need to know the registry value to collect the keys. This is available for virtual machine profiles only.
image:2052-reg.png[]
. Click *Event Log* to specify event log entries to collect.
. From the *Event Log Entry* area, complete the fields to add a type of event log entry. You can type in a specific message to find in *Filter Message*.
In *Level*, set the value for the level of the entry and above. Specify the *Source for the entry*. Finally, set the # (number) of days that you want to collect event log entries for.
If you set this to 0, it will go as far back as there is data available.
image:2054.png[]
. Click *Add*.

[[editing-an-analysis-profile]]
===== Editing an Analysis Profile

To edit an analysis profile:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Analysis Profiles*.
. Check the analysis profile you want to edit.
. Click image:1851.png[] (*Edit the selected Analysis Profiles*).
. Make any changes.
. Click *Save*.

The changes are added to the analysis profile. The virtual machines or hosts must be re-analyzed to collect the new or modified information.

[[copying-an-analysis-profile]]
===== Copying an Analysis Profile

To copy an analysis profile:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Analysis Profiles*.
. Check the analysis profile you want to copy.
. Click image:1859.png[] (*Copy the selected Analysis Profiles*).
. Type a new *Name* and *Description*.
. Make required changes.
. Click *Add*.

[[setting-a-default-analysis-profile]]
===== Setting a Default Analysis Profile

If you want to set an analysis profile to be used for all virtual machines, you can create a default profile.

To create a default analysis profile:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Analysis Profiles*.
. Click on the analysis profile you want to set as the default.
. Click image:1851.png[] (*Edit the selected Analysis Profile*).
. For a virtual machine profile, enter `default` in lower case in *Name*. For a host profile, enter host default.
image:set_default_analysis_profile.png[]
. Click *Save*.


[[zones]]
==== Zones

You can organize your {product-title} Infrastructure into zones to configure failover and isolate traffic. A provider that is discovered by a server in a specific zone gets monitored and managed in that zone.
All jobs, such as a SmartState Analysis or VM power operation, dispatched by a server in a specific zone can get processed by any {product-title} appliance assigned to that same zone.

Zones can be created based on your own environment. You can make zones based on geographic location, network location, or function. When first started, a new server is put into the default zone.

Suppose you have four {product-title} appliances with two in the East zone, appliances A and B, and two in the West zone, appliances C and D. VC East is discovered by one of the {product-title} appliances in the {product-title} Eastern zone.
If Appliance A dispatches a job of analyzing twenty virtual machines, this job can be processed by either Appliance A or B, but not C or D.

[NOTE]
======
Only users assigned the super administrator role can create zones. There must always be at least one zone. The *Default Zone* is provided and cannot be deleted.
======

[[creating-a-zone]]
===== Creating a Zone

To create a zone:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Zones*.
. Click image:1847.png[] (*Configuration*), and image:plus_green.png[] (*Add a new Zone*) to create a zone.
. In the *Zone Information* area, type in a *Name* and *Description* for the new zone.
image:2057.png[]
. Use *SmartProxy Server IP* to specify the IP address of the server that you want SmartProxies installed in this zone to report to. If this is not set, then the IP address of the server that deployed the SmartProxy is used. This does not apply to embedded SmartProxies.
. Optionally, you can configure *NTP servers* for the entire zone in the NTP Servers area. These settings will be used if the NTP servers have not been set for the appliance in the menu:Operations[Server] page.
. In the menu:Credentials[Windows Domain] area, type in Windows domain credentials to be able to collect running processes from Windows virtual machines that are on a domain.
image:2058.png[]
. In the *Settings* area, set the number for *Max Active VM Scans*. The default is `Unlimited`.
. Click *Save*.

[[deleting-a-zone]]
===== Deleting a Zone

To delete a zone:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Zones*.
. Click the zone you want to remove.
+
[NOTE]
======
You cannot delete a zone if there are servers assigned to it.
======
+
. Click image:1847.png[] (*Configuration*), then click image:gui_delete.png[] (*Delete this Zone*).
. Click *OK* to confirm.

[[editing-a-zone]]
===== Editing a Zone

To edit a zone:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Zones*.
. Click the zone you want to edit.
. Click image:1847.png[] (*Configuration*), then click image:1851.png[] (*Edit this Zone*).
. Make the required changes.
. Click *Save*.

[[adding-smartproxy-affinity-to-a-zone]]
===== Adding SmartProxy Affinity to a Zone:

Enable SmartProxy Affinity for zones containing servers with the SmartProxy role to run a SmartState Analysis.

To add SmartProxy Affinity to a zone:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Zones*.
. Click the zone in which you want to enable SmartProxy Affinity.
. Click the *SmartProxy Affinity* tab and click the appropriate server.
. Click *Save*.

[[servers]]
==== Servers

Server settings enables you to control how each {product-title} server operates including authentication, logging, and email.
If you have multiple servers in your environment that are reporting to one central VMDB, then you can edit some of these settings from the console by specifying which server you want to change.

[NOTE]
======
The server selection options are only available if you have multiple servers sharing one VMDB.
======

[[changing-server-settings]]
===== Changing Server Settings

To change server settings:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Zones*.
. Click the zone where the {product-title} server is located.
. In the *Servers* area, click on the {product-title} server.
. Click *Server*.
. Make any required changes.
. Click *Save*.

[[basic-information-settings]]
====== Basic Information Settings
image:2059.png[]

* Use *Company Name* (maximum 20 characters) to customize the interface with your company's name. You will see the company name when you are viewing or modifying the tags of an infrastructure object or virtual machine.
* Specify the *Appliance Name* (maximum 20 characters) you want displayed as the appliance that you are logged into. You will see this in the upper right corner of the interface with the name of the consoles logged on user.
* Use *Zone* to isolate traffic and provide load balancing capabilities. Specify the zone that you want this {product-title} appliance to be a member of. At startup, the zone is set to default.
* Use *Appliance Time Zone* to set the time zone for this server.
+
[NOTE]
======
This is the time zone used when created scheduled analyses. This is not the same as the *Time Zone* parameter, which is found by navigating to the settings menu, then *My Settings*, then exploring the *Display Settings* area, and is the time zone displayed in the console.
======
+
* Use *Default Locale* to specify the default language for this server.

[[server-control-settings]]
====== Server Control Settings

A server role defines what a server can do. Red Hat recommends that Database Operations, Event Monitor, Reporting, Scheduler, SmartState Analysis, User Interface, Provider Inventory, Provider Operations, and Web Services be enabled on at least one server in each zone.
These roles are enabled by default on all servers.

* Use *Default Repository SmartProxy* to set the SmartProxy from which you refresh your virtual machine repositories. This host must have access to your repositories to analyze its virtual machines.

[NOTE]
======
* Only super administrators can change server roles.
* If you are using more than one {product-title} appliance, be sure to set this on all of the appliances.
======

[[server-roles]]
====== Server Roles

[NOTE]
====
* Server roles that are in an active/active high availability configuration _(load balancing and failover protection)_ are active in more than one location; whereas, roles that are in an active/passive _(primary/secondary in the case of {product-title_short})_ high availability configuration _(failover protection)_, if more than one {product-title_short} server in a specific zone or region has this role, only one will be active _(primary)_ at a time and a failover has to occur to the passive _(secondary)_ appliance with that role.
* For information on region and zone diagnostics and server role priorities, see xref:diagnostics[].
====

[width="100%",cols="50%,70%a,30%,55%",options="header",]
|=======================================================================
|Server Role|Description|Zone or Region Aware|Primary/Secondary or Active/Active
|Automation Engine|Use this role if you want to use this {product-title_short} server to process automation tasks.|N/A|Active/Active
|Capacity and Utilization Coordinator|The Capacity & Utilization Coordinator role checks to see if it is time to collect data, somewhat like a scheduler. If it is time, a job is queued for the Capacity and Utilization Data Collector. The coordinator role is required to complete Capacity and Utilization data collection. If more than one {product-title_short} server in a specific zone has this role, only one will be active at a time.|Zone|Primary/Secondary
|Capacity & Utilization Data Collector|The Capacity & Utilization Data Collector performs the actual collection of capacity and utilization data. This role has a dedicated worker, and there can be more than one {product-title_short} server with this role in a zone.|Zone|Active/Active
|Capacity & Utilization Data Processor|The Capacity & Utilization Data Processor processes all of the data collected, allowing {product-title_short} to create charts. This role has a dedicated worker, and there can be more than one {product-title_short} server with this role in a zone.|Zone|Active/Active
|Database Operations|Use Database Operations to enable this {product-title_short} server to run database backups or garbage collection.|Zone|Active/Active
|Event Monitor|This role is enabled by default and provides the information shown in timelines. The Event Monitor is responsible for the work between the {product-title_short} server and your providers. It starts 2 workers for each provider. One worker, the monitor, is responsible for maintaining a connection to a provider, catching events, and putting them on the {product-title_short} message queue for processing. The second worker, the handler, is a message queue worker responsible for delivering only those messages for a provider. You should have at least one of these in each zone.|Zone|Primary/Secondary
|Git Repository|The Git Repositories Owner server role supports importing domains into automate from a git repository. This feature is available from the Automate > Import/Export screen in the {product-title_short} user interface.|Region|Primary/Secondary
|Notifier|Use this role if you will be using {product-title_short} Control or Automate to forward SNMP traps to a monitoring system or send e-mails. See <<Configuring SNMP>> for details on creating SNMP alerts. If more than one {product-title_short} server in a specific region has this role, only one will be active at a time.|Region|Primary/Secondary
|Provider Inventory|This role is enabled by default. This role is responsible for refreshing provider information including EMS, hosts, virtual machines, and clusters, and is also responsible for capturing datastore file lists. If more than one {product-title_short} server in a specific zone has this role, only one will be active at a time.|Zone|Primary/Secondary
|Provider Operations|This role is enabled by default. This role sends stop, start, suspend, shutdown guest, clone, reconfigure, and unregister to the provider, directly from the console or through a policy action if you have {product-title_short} Control. More than one {product-title_short} server can have this role in a zone.|Zone|Active/Active
|RHN Mirror|An appliance with RHN Mirror enabled acts as a server containing a repository with the latest {product-title_short} packages. This also configures other appliances within the same region to point to the chosen RHN Mirror server for updates. This provides a low bandwidth method to update environments with multiple appliances.|N/A|Active/Active
|Reporting|This role is enabled by default. The Reporting role specifies which {product-title_short} servers can generate reports. If you do not have a {product-title_short} server set to this role in a zone, then no reports can be generated in that zone. You should have at least one of these in each zone.|Zone|Active/Active
|Scheduler|This role is enabled by default. The Scheduler sends messages to start all scheduled activities such as report generation and SmartState analysis. This role also controls all system schedules such as capacity and utilization data gathering. One server in each region must be assigned this role or scheduled {product-title_short} events will not occur. If more than one {product-title_short} server in a specific region has this role, only one will be active at a time.|Region|Primary/Secondary
|SmartProxy|Enabling the SmartProxy role turns on the embedded SmartProxy on the {product-title_short} server. The embedded SmartProxy can analyze virtual machines that are registered to a host and templates that are associated with a provider. To provide visibility to repositories, install the SmartProxy on a host from the {product-title_short} console. This SmartProxy can also analyze virtual machines on the host on which it is installed. Enabling the SmartProxy role on an appliance requires selecting the SmartProxy Affinity for a zone to run a SmartState Analysis. By default, no selections are enabled under SmartProxy Affinity.|Zone|Active/Active
|SmartState Analysis|This role is enabled by default. The SmartState Analysis role controls which {product-title_short} servers can control SmartState Analyses and process the data from the analysis. You should have at least one of these in each zone.|Zone|Active/Active
|User Interface|This role is enabled by default. The Web Services role must also be enabled with this role to log into the user interface, as the User Interface role queries the API to receive tokens for login. Uncheck User Interface if you do not want users to be able to access this {product-title_short} server using the {product-title_short} console. For example, you may want to turn this off if the {product-title_short} server is strictly being used for capacity and utilization or reporting generation. More than one {product-title_short} server can have this role in a zone.|Zone|Active/Active
|Web Services|This role is enabled by default. The Web Services roles provides API access and must be enabled if the User Interface role is enabled to log into the user interface. You can also enable the Web Services role to provide API-only access to the server. Uncheck Web Services to stop this {product-title_short} server from acting as a web service provider. More than one {product-title_short} server can have this role in a zone.|N/A|Active/Active
|Websocket|This role enables starting or stopping websocket workers required for proxying remote consoles.|N/A|Active/Active
|=======================================================================


[[vmware-console-settings]]
====== VMware Console Settings

If you are using the {product-title} Control feature set, then you have the ability to connect to a Web console for virtual machines that are registered to a host. To use this feature, you must have VNC installed, the appropriate version of the VMware MKS plug-in or the appropriate VMRC viewer installed in your Web browser.

[NOTE]
======
* You are responsible for installing the correct version for your virtual infrastructure. See vendors documentation for information. After installing the appropriate software or version, you must specify which version you are using in the {product-title} configuration settings.
* To edit the VMware MKS plug-in settings, you must have the super administrator role.
======

image:2061.png[]

* If you select *VNC*, type in the port number used. This port must be open on the target virtual machine and the VNC software must be installed there. On the computer that you are running the console from, you must install the appropriate version of Java Runtime if it is not already installed.
* If you select *VMware MKS* plug-in, select the appropriate version.
* If using *VMware VMRC* plug-in, be sure that you have fulfilled the requirements. The correct version of the VMRC plug-in from VMware must be installed on the client computer.
To do this, log into the Virtual Center Web Service and attempt to open a virtual machine console. This should prompt you to install the required plug-in.
The VSphere Web Client must be installed on VC version 5, and the provider must be registered to it. For Virtual Center version 4, the VMware VirtualCenter Management Webservice must be running.

[[ntp-servers-settings]]
====== NTP Servers Settings
In the *NTP Servers* area, you can specify the NTP servers to use as source for clock synchronization here. The NTP settings specified here will override Zone NTP settings. Enter one NTP server hostname or IP address in each text box.

[[configuring-snmp]]
====== Configuring SNMP

You can use Simple Network Management Protocol (SNMP) traps to send alerts for various aspects of a {product-title} environment.

*Requirements*

* Configure your SNMP management station to accept traps from {product-title} appliances. Consult your management station's documentation.
* Each appliance that could process SNMP traps must have the `snmpd` and `snmptrapd` daemons running.
* The region where the appliances are located must have the `Notifier` role enabled and the failover role priority set.

To enable the `snmpd` and `snmptrapd` daemons

. Access each SNMP processing appliance using SSH.
. Set the SNMP daemons to run on start up:
+
------
# chkconfig --level 2345 snmpd on
# chkconfig --level 2345 snmptrapd on
------
+
. The daemons run automatically when the appliance is restarted, but must be started manually now.
+
------------
# service snmpd start
# service snmptrapd start
------------
+


To enable the notifier role:

. Access each SNMP processing appliance using their web interfaces.
. From the settings menu, select menu:Configuration[Settings].
. Select the zone where the EVM server is located, and select the EVM server.
. In the *Server Control* area, set the *Notifier* server role option to `ON`.
. Click *Save*.

To set the failover priority role:

. From the settings menu, select menu:Configuration[Diagnostics].
. Select the zone where the EVM server is located.
. Click *Roles by Servers* or *Servers by Roles* to view your servers.
. In the *Status of Roles for Servers* in *Zone Default* Zone area, click the role that you want to set the priority for.
. Click image:1847.png[](*Configuration*), and image:2097.png[](*Promote Server*) to make this the primary server for this role.

[[outgoing-smtp-email-settings]]
====== Outgoing SMTP Email Settings

To use the email action in {product-title}, set an email address to send emails from.

[NOTE]
======
To be able to send any emails from the server, you must have the Notifier server role enabled. You can test the settings without the role enabled.
======

image:OutgoingSMTP.png[]

* Use *Host* to specify the host name of the mail server.
* Use *Port* to specify the port for the mail server.
* Use *Domain* to specify the domain name for the mail server.
* Set *Start TLS Automatically* on `ON` if the mail server requires TLS.
* Select the appropriate *SSL Verify Mode*.
* Use the *Authentication* drop down to specify if you want to use `login`, `plain`, or no authentication.
* Use *User Name* to specify the user name required for login authentication.
* Use *Password* to specify the password for login authentication.
* Use *From E-mail Address* to set the address you want to send the email from.
* Use *Test E-mail Address* if you want to test your email settings. Click *Verify* to send a test email.

[[web-services-settings]]
====== Web Services Settings

Web services are used by the server to communicate with the SmartProxy.

image:2064.png[]

* Set *Mode* to invoke to enable 2-way Web services communication between the {product-title} appliance and the SmartProxy. Set *Mode* to disabled to use Web services from the SmartProxy to the {product-title} appliance only. When the {product-title} appliance has work for the SmartProxy, the work will be placed in a queue in the VMDB. The work will be completed either when the {product-title} appliance is able to contact the SmartProxy or when the next SmartProxy heartbeat occurs, whichever comes first.
* If *Web Services* are enabled, you have the option to use *ws-security*.

[[logging-settings]]
====== Logging Settings

image:2065.png[]

* Use *Log Level* to set the level of detail you want in the log. You can select from *fatal*, *error*, *warn*, *info*, and *debug*. The default setting is 'info'.

[[custom-support-url-settings]]
====== Custom Support URL Settings

image:2066.png[]

* Use *URL* to specify a specific URL that you want to be accessible from the *About Product Assistance* area.
* Use *Description* to set a label for the *URL*.

[[authentication]]
===== Authentication

Use the *Authentication* tab to specify how you want users authenticated on the console. You can use the VMDB or integrate with LDAP, LDAPS, Amazon, or an external IPA server.

ifdef::cfme[]
[NOTE]
====
See https://access.redhat.com/documentation/en-us/red_hat_cloudforms/4.6-beta/html-single/managing_authentication_for_cloudforms/[_Managing Authentication_] for information on configuring different types of authentication for {product-title_short}.
====
endif::cfme[]


[[changing-authentication-settings]]
====== Changing Authentication Settings

To change authentication settings:

. From the settings menu, select *Configuration*.
. Click the *Settings* accordion, then click *Zones*.
. Click the zone where the server is located.
. Click the server.
. Click the *Authentication* tab.
. Use *Session Timeout* to set the period of inactivity before a user is logged out of the console.
. Set the authentication method in *Mode*.
. Click *Save*.

////

Note: commenting all of this content out as it's now in the Managing Authentication guide. Leaving in this guide temporarily in case it is needed for any reason. All xrefs to this section have been changed to URLs to the new guide. We can probably delete this content entirely after 4.6 is released.

[[ldap_settings]]
====== Configuring LDAP Authentication

If you choose LDAP or LDAPS as your authentication mode, the required parameters are exposed under *LDAP Settings*. Be sure to validate your settings before saving them.

To configure {product-title_short} to use LDAP for authentication, complete the following steps:

. From the settings menu, select *Configuration*. Select the *Authentication* tab.
. Select a *Session Timeout* to set the period of inactivity before a user is logged out of the console.
. Select *LDAP* or *LDAPS* from the *Mode* list. This exposes additional required parameters under *LDAP Settings*.
. Configure your *LDAP Settings* (the following example configures a Red Hat Identity Management LDAP server):
* Use *LDAP Host Names* to specify the fully qualified domain names of your LDAP servers. {product-title_short} will search each host name in order until it finds one that authenticates the user. Note, {product-title_short} supports using a maximum of 3 possible *LDAP Host Names*.
* Use *LDAP Port* to specify the port for your LDAP server. The default is 389 for LDAP and 636 for LDAPS.
* From the *User Type* list, select *User Principal Name* to type the user name in the format of user@domainname. Select *Email Address* to login with the users email address.
Select *Distinguished Name* (CN=<user>) or *Distinguished Name* (UID=<user>) to use just the user name, but be sure to enter the proper *User Suffix* for either one. Choose the correct *Distinguished Name* option for your directory service implementation.
* Specify the *User Suffix*, such as acme.com for *User Principal Name* or cn=users,dc=acme,dc=com for *Distinguished Name*, in *Base DN*.
* From the *User Type* list, select one of the following and configure the values for your LDAP server:
** *User Principal Name*: Type the user name in the format of user@domainname, for example, `dbright@acme.com`. (In this case, the user would log on as `dbright`.)
** *Email Address*: Logs in with the user's email address.
** *Distinguished Name* (CN=<user>): Uses the common name for the user. Be sure to enter the correct *User Suffix* and *Distinguished Name* option for your directory service implementation: for example, `cn=dan bright,ou=users,dc=acme,dc=com`. (The user logs on as `dan bright`.)
** *Distinguished Name* (UID=<user>): Uses the user ID (UID). Be sure to enter the correct *User Suffix* and *Distinguished Name* option for your directory service implementation: for example, `uid=dan bright,ou=users,dc=acme,dc=com`. (The user logs on as `dan bright`.)
** *SAM Account Name*: User logon for Active Directory clients and servers using legacy Windows versions.
* Specify the *User Suffix*, such as `acme.com` for *User Principal Name* or `cn=users,dc=acme,dc=com` for *Distinguished Name*, in *Base DN*.
+
[NOTE]
======
The `ldapsearch(1)` command can be used to get details of your LDAP settings. To get details related to a specific user, run:

  # ldapsearch -D "cn=directory manager" -H ldap://www.acme.com:389 -b "dc=acme,dc=com" -s sub "(objectclass=*)" -w password | grep -i dbright

To search for your LDAP server's distinguished name (DN) values, run:

  # ldapsearch -D "cn=directory manager" -H ldap://www.acme.com:389 -b "dc=acme,dc=com" -s sub "(objectclass=*)" -w password
======
+
.Example: LDAP Configuration
image:LDAP-authentication-full.png[]
+
. Configure your *Role Settings*:
In both LDAP and LDAPS, you can use groups from your directory service to set the role for the authenticated LDAP user. The LDAP user must be assigned one of the account role groups. See xref:assigning_account_roles_using_ldap_groups[] for more information.
* For LDAP users not belonging to a group:
** Select a {product-title_short} group from the *Default Group for Users* list. This default group can be used for all LDAP users who use LDAP for authentication only. Do not select *Get User Groups from LDAP*, which will hide the *Default Group for Users* option.
* For LDAP users belonging to a group:
** Check *Get User Groups from LDAP* to retrieve the user's group membership from LDAP. This is used for mapping a user's authorization to a {product-title_short} role. This requires group names on the LDAP server to match {product-title_short} group names.
+
[IMPORTANT]
======
If you do not check *Get User Groups from LDAP*, the user must be defined in the VMDB using the console where the User ID is the same as the user's name in your directory service typed in lowercase.
======
** Check *Get Roles from Home Forest* to use the LDAP roles from the LDAP user's home forest. This will allow you to discover groups on your LDAP server and create {product-title_short} groups based on your LDAP server's group names. Any user logging in will be assigned to that group. This option is only displayed when *Get User Groups from LDAP* is checked.
** Check *Follow Referrals* to look up and bind a user that exists in a domain other than the one configured in the LDAP authentication settings.
** Specify the user name to bind to the LDAP server in *Bind DN*. This user must have read access to all users and groups that will be used for {product-title_short} authentication and role assignment, for example , a service account user with access to all LDAP users.
** Enter the password for the Bind DN user in *Bind Password*.
+
. Click *Validate* to verify your settings.
. Click *Save*.

LDAP authentication is now configured in your CloudForms environment.

To assign account roles using LDAP groups, see xref:assigning_account_roles_using_ldap_groups[].


[[trusted-forests]]
====== Trusted Forests

Optionally, if a user has group memberships in another LDAP forest, specify the settings to access the memberships in the trusted forest.

When trusted forests are added to the authentication configuration, they are used only for finding groups that a user is a member of. {product-title_short} will first collect all of the user's groups from the primary LDAP directory. Then it will collect any additional groups that the user is a member of from all of the configured forests.

The collected LDAP groups are used to match, by name, against the groups defined in {product-title_short}. The user must be a member of at least one matching LDAP group to be successfully authenticated.

To add settings for a trusted forest:

. From the settings menu, select *Configuration*.
. Click the *Settings* accordion, then click *Zones*.
. Click the *Zone* where the server is located.
. Click the *Server*.
. Click the *Authentication* tab.
. Check *Get User Groups from LDAP*, and enter all items in the *Role Settings* area.
. In the *Trusted Forest Settings* area, click image:green-plus.png[](*Click to add a new forest*).
. Enter the *LDAP Host Name*, select a *Mode*, and enter an *LDAP Port*, *Base DN*, *Bind DN*, and *Bind Password*.
. Click *Save*.

After adding other trusted LDAP forests, you can then change the order in which {product-title_short} looks up the forests for authentication. For instructions, see xref:ldap_lookup_priority[].


[[assigning_account_roles_using_ldap_groups]]
====== Assigning {product-title_short} Account Roles Using LDAP Groups

After configuring LDAP authentication as described in xref:ldap_settings[], you can associate {product-title_short} account roles with your LDAP users. The LDAP server defines the groups and users for {product-title_short}, while {product-title_short} defines the account roles, and maps the roles to the privileges the LDAP user has.

There are two ways to associate your LDAP groups with {product-title_short} account roles:

* Create groups in {product-title_short} that match your existing LDAP groups by name, and assign the groups account roles; or
* Create groups on your LDAP server based on the default account roles in {product-title_short}.

The users in your LDAP groups then inherit the {product-title_short} account roles for the LDAP group(s) they are in.

The authentication process then happens as such:

. `LDAPuser1` attempts to log into {product-title_short}, so {product-title_short} queries the LDAP server to verify it knows `LDAPuser1`.
. The LDAP server then confirms that it knows `LDAPuser1`, and provides information about the LDAP groups `LDAPuser1` belongs to: `Group1`.
. {product-title_short} then looks up `Group1`, and discovers that `Group1` is associated with `Role1`.
. {product-title_short} then associates `LDAPuser1` with `Group1` in {product-title_short}, and then allows the user to perform tasks allowable by that role.


====== Using Existing LDAP Groups to Assign Account Roles

This section provides instructions for mapping your existing LDAP groups to account roles in {product-title_short}. As a result, the users in the LDAP group will then be assigned to the CFME roles associated with that group.

. From the settings menu, select *Configuration*.
. Click the *Access Control* accordion, then click *Groups*.
. Click image:1847.png[] (*Configuration*), and image:plus_green.png[] (*Add a new Group*) to create a group.
. There are two ways to specify the group to use:
* In the *Description* field, enter the (common name (`cn`) for your existing LDAP group assigned to users requiring access to {product-title_short}.
*  Select *Look Up LDAP Groups* to find a list of groups assigned to a specific user in LDAP, then use the *LDAP Group for User* list to choose a group.
.. In *User to Look Up*, enter the common name (`cn`) for a user in your LDAP group.
.. Enter the *Username*.
.. In *Password*, enter the user's password. Click *Retrieve*.
. Select a *Role* to map to the group.
. Select a *Project/Tenant* to map to the group.
+
image:Assign_LDAP-Roles.png[]
+
. Select any filters to apply to what this group can view in the *Assign Filters* area:
.. In the *My Company Tags* tab, select tags to limit the user to items containing those tags. The items that have changed show in a blue italicized font.
.. In the *Host & Clusters* tab, select the host and clusters to limit the user to. The items that have changed show in a blue italicized font.
image:2093.png[]
.. In the *VMs & Templates* tab, select the folders created in your virtual infrastructure to limit the user to. The items that have changed show in a blue italicized font.
. Click *Add*.

To configure the LDAP group lookup priority, see xref:ldap_lookup_priority[].


====== Using Groups Named by {product-title_short} to Assign Account Roles

You can also configure access control for LDAP users by creating groups on your LDAP server based on {product-title_short} account roles.

Your LDAP group names must match the account role names in {product-title_short}. The LDAP users in that group are then automatically assigned to that specific account role.

. In your LDAP directory service, define a distribution group for one or more of the account roles with the names shown in the table below.
This group must be in the LDAP directory source you specified for the server. See xref:ldap_settings[].
+
.Account Role and Directory Service Group Names

[width="100%",cols="50%,50%",options="header",]
|=======================================================================
|Directory Service Distribution Group Name|Account Role
|EvmGroup-administrator|Administrator
|EvmGroup-approver|Approver
|EvmGroup-auditor|Auditor
|EvmGroup-consumption_administrator|Consumption Administrator
|EvmGroup-container_administrator|Container Administrator
|EvmGroup-container_operator|Container Operator
|EvmGroup-desktop|Desktop
|EvmGroup-operator|Operator
|EvmGroup-security|Security
|EvmGroup-super_administrator|Super Administrator
|EvmGroup-support|Support
|EvmRole-tenant_administrator|Tenant Administrator
|EvmRole-tenant_quota_administrator|Tenant Quota Administrator
|EvmGroup-user|User
|EvmGroup-user_limited_self_service|User Limited Self Service
|EvmGroup-user_self_service|User Self Service
|EvmGroup-vm_user|Vm User
|=======================================================================
+
. Make each user of your directory service that you want to have access to {product-title_short} a member of one of these groups.
. From the settings menu, select *Configuration*.
. Click the *Settings* accordion, then select your server under *Zones*.
. Click the *Authentication* tab and enable *Get User Groups from LDAP* after typing in all of the required LDAP authentication settings. See xref:ldap_settings[].


[[ldap_lookup_priority]]
====== Configuring Lookup Priority for LDAP Groups

{product-title_short} can have multiple LDAP groups configured, which the appliance will attempt to authenticate with one by one until it succeeds. The lookup priority of these groups can be rearranged; to configure the order in which {product-title_short} looks up LDAP groups:

[NOTE]
====
On initial login, a user's _current group_ assignment is the highest priority group. User group membership, on subsequent logins, is set as the last assigned group from the prior session.
====

. From the settings menu, select *Configuration*.
. Click on the *Access Control* accordion, then click *Groups*.
. Click image:1847.png[] (*Configuration*), and image:plus_green.png[] (*Edit Sequence of User Groups for LDAP Look Up*) to prioritize which group a user will default to if LDAP returns multiple matching groups.
. Select one or more consecutive groups and use the arrow buttons to move the user group higher or lower in priority.
. Click *Save*.

====== Testing LDAP Configuration

To test that your LDAP or LDAPS group configuration is working correctly with {product-title_short}:

. Log out of the {product-title_short} user interface.
. Log back in as an LDAP user that is assigned to one or more of the matching groups.
. Change groups by clicking on the user dropdown menu on the top right of the user interface. The dropdown list will show the groups the user is authorized for.

You can also check the logs in `/var/www/miq/vmdb/log/audit.log` or `/var/www/miq/vmdb/log/evm.log` to verify your LDAP configuration is working correctly.

First, run the following command in a terminal to view the log messages in real time:

  tail -f /var/www/miq/vmdb/log/audit.log

Then log into the CloudForms user interface as an LDAP user, while checking `/var/www/miq/vmdb/log/audit.log` for updated status, success, or failure messages. Alternatively, you can test your LDAP configuration by viewing the logs in `/var/www/miq/vmdb/log/evm.log` with `grep`, which are more verbose.


.Troubleshooting LDAP configuration

To test a problematic {product-title_short} LDAP configuration, run the following to see if the user been pulled from LDAP with the right group. For example:

  # ldapsearch -x -H ldap://ldap-example:389 -LLL \ -b "ou=people,dc=example,dc=com" -s sub \ -D "ui=:userid,ou=People,dc=example,dc=com" -w :password \ "(objectclass=organizationalPerson)

To test if the user belongs to right group, include one of the following lines in the `ldapsearch` command:

  (&(objectClass=user)(sAMAccountName=yourUserName) (memberof=CN=YourGroup,OU=Users,DC=YourDomain,DC=com))

or

  -b "ou=groups, dc=example,dc=com"



[[amazon_settings]]
====== Amazon Settings

If you choose Amazon as your authentication mode, required parameters are exposed under *Amazon Primary AWS Account Settings* for *IAM*. Be sure to validate your setting before saving them.

* Type in an *Access Key* provided by your Amazon account.
* Type in a *Secret Key* provided by your Amazon account.

Users logging into {product-title_short} with Amazon authentication enter their own IAM Access Key as the username and IAM Secret Key as the password.
Amazon users must be added as a {product-title_short} user or belong to an IAM user group added to the list of {product-title_short} groups.


[[external_ipa_auth]]
====== External Authentication (httpd)

When external authentication is enabled, users can log in to the {product-title_short} appliance using their IPA server credentials. The appliance creates user accounts automatically and imports relevant information from the IPA Server.

The appliance contains IPA client software for connecting to IPA servers, but it is not configured by default. External authentication is enabled by first configuring it in the web interface, then in the console.
Disabling external authentication and returning to internal database authentication also requires steps in both the web interface and the console.

*Requirements*

* For an appliance to leverage an IPA Server on the network, both the appliance and the IPA server must have their clocks synchronized or Kerberos and LDAP authentication fail.
* The IPA Server must be known by DNS and accessible by name. If DNS is not configured accordingly, the hosts files need to be updated to reflect both IPA server and the appliance on both virtual machines.
* For users to log in to the appliance using IPA server credentials, they must be members of at least one group on the IPA server which is also defined in the appliance. Navigate to the settings menu, then menu:Configuration[Access Control > Groups] to administer groups.

*Configuring the Appliance for External Authentication*

To configure the appliance for external authentication, first set up authentication using the web interface, then using the console.

Using the web interface:

. Log in to the web interface as an administrative user.
. Navigate to the settings menu, then menu:Configuration[Access Control > Zone > Server > NTP Servers] or use the hosting provider of the virtual machine to synchronize the appliance's time with an NTP server.
. Navigate to the settings menu, then menu:Configuration[Authentication].
. Select a *Session Timeout* if required.
. Select *External (httpd)* in the *Mode list*.
. Select *Enable Single Sign-On* to allow single sign-on using Kerberos tickets from client machines that authenticate to the same IPA server as the appliance.
. In the *Role Settings* area, select *Get User Groups* from *External Authentication (https)*.
. Click *Save*.

Using the console:

. Log in to the appliance console using the user name `admin`.
. The summary screen displays:
+
------
External Auth:  not configured
------
+
. Press Enter.
. Enter `10` to select Configure External Authentication (httpd).
. Enter the fully qualified hostname of the IPA Server, for example _ipaserver.test.company.com_.
. Enter the IPA server domain, for example _test.company.com_.
. Enter the IPA server realm, for example _TEST.COMPANY.COM_.
. Enter the IPA server principal, for example _admin_.
. Enter the password of the IPA server principal.
. Enter `y` to proceed.

[NOTE]
============
If any of the following conditions are true, configuration fails:

* The IPA server is not reachable by its FQDN
* The IPA server cannot reach the appliance by its FQDN
* The time is not synchronized between the appliance and the IPA server
* The IPA server admin password is entered incorrectly
============

*Reverting to Internal Database Authentication*

To revert to internal database authentication, first configure authentication using the web interface, then using the console.

Using the web interface:

. Log in to the web interface as an administrative user.
. Navigate to the settings menu, then menu:Configuration[Authentication].
. Select *Database* in the Mode list.
. Click *Save*.

Using the console:

. Log in to the appliance console using the user name `admin`.
. The summary screen displays:
+
------
External Auth:  IPA.server.FQDN
------
+
. Press `Enter`.
. Enter `10` to select Configure External Authentication (httpd). The currently configured IPA server hostname and domain are displayed.
. Enter `y` to un-configure the IPA client.

*Optional Configuration Using the Appliance Console CLI*

In addition to using the appliance console, external authentication can optionally be configured and un-configured using the appliance console command line interface.

Appliance console CLI command and relevant options include:

------
/bin/appliance_console_cli --host <appliance_fqdn>
                           --ipaserver <ipa_server_fqdn>
                           --iparealm <realm_of_ipa_server>
                           --ipaprincipal <ipa_server_principal>
                           --ipapassword <ipa_server_password>
                           --uninstall-ipa
------

 *--host*:

updates the hostname of the appliance. If you performed this step using the console and made the necessary updates made to `/etc/hosts` if DNS is not properly configured, you can omit the `--host` option.

 *--iparealm*:

if omitted, the `iparealm` is based on the domain name of the `ipaserver`.

 *--ipaprincipal*:

if omitted, defaults to admin.

.Configuring External Authentication
====
----
$ ssh root@appliance.test.company.com
[appliance]# /bin/appliance_console_cli --host appliance.test.company.com \
                                      --ipaserver ipaserver.test.company.com \
                                      --iparealm TEST.COMPANY.COM \
                                      --ipaprincipal admin \
                                      --ipapassword smartvm1
----
====

.Reverting to Internal Database Authentication
====
----
$ ssh root@appliance.test.company.com
[appliance]# /bin/appliance_console_cli --uninstall-ipa
----
====

[[external-authentication-SAML]]
====== Configuring External Authentication Using SAML

This procedure outlines how to manually configure an appliance to use SAML external authentication. While other SAML identity providers can be used with {product-title_short}, this procedure covers using Red Hat Single Sign-On (SSO) 7.0, which is implemented using the Apache HTTP server's `mod_auth_mellon` module.

To enable external authentication using SAML, complete the following steps to configure your HTTP server, then your {product-title_short} appliance.

[NOTE]
======
The current SAML implementation only secures the {product-title_short} appliance’s web administrative user interface with SAML. The REST API and self service user interface do not currently support SAML.
======

*Requirements*

The following is required in order to enable SAML authentication to the appliance:

* A CloudForms 4.6 appliance
* A SAML identity provider (e.g. Red Hat Single Sign-On (SSO) 7.0 or later)

[[configure-apache-SAML]]
*Configuring the HTTP Server for SAML*

The Apache HTTP server first must be configured to work with SAML authentication. All SAML-related certificates and keys are accessed from `/etc/httpd/saml2/`.

. Log into the {product-title_short} appliance as root using SSH, and create the `/etc/httpd/saml2/` directory:
+
------
# mkdir -p /etc/httpd/saml2
------
+
. Copy the `httpd` remote user and SAML template configuration files to the appliance:
+
------
# TEMPLATE_DIR="/opt/rh/cfme-appliance/TEMPLATE"
# cp ${TEMPLATE_DIR}/etc/httpd/conf.d/manageiq-remote-user.conf /etc/httpd/conf.d/
# cp ${TEMPLATE_DIR}/etc/httpd/conf.d/manageiq-external-auth-saml.conf /etc/httpd/conf.d/
------
+
[NOTE]
============
The following are notable SAML configuration defaults in the `manageiq-external-auth-saml.conf` file:

* Identity Provider Files (i.e. Red Hat SSO)
** Metadata File: `/etc/httpd/saml2/idp-metadata.xml`

* Service Provider Files (i.e. `mod_auth_mellon`)
** Private Key File: `/etc/httpd/saml2/miqsp-key.key`
** Certificate File: `/etc/httpd/saml2/miqsp-cert.cert`
** Metadata File: `/etc/httpd/saml2/miqsp-metadata.xml`

Other `mod_auth_mellon` parameters, such as endpoints and protected URLs, must not be modified as the appliance expects them to be defined as such.
============
+
. Generate the service provider files on the appliance using the Apache HTTP server's `mod_auth_mellon` command `mellon_create_metadata.sh`:
+
------
# cd /etc/httpd/saml2
# /usr/libexec/mod_auth_mellon/mellon_create_metadata.sh https://<miq-appliance> https://<miq-appliance>/saml2
------
+
The `mellon_create_metadata.sh` script creates file names based on the appliance URL.
+
. Rename the files created by the `mellon_create_metadata.sh` script to match the expected file names from the `manageiq-external-auth-saml.conf` file:
+
------
# mv https_<miq-appliance>.key  miqsp-key.key
# mv https_<miq-appliance>.cert miqsp-cert.cert
# mv https_<miq-appliance>.xml  miqsp-metadata.xml
------
+
. Now that the service provider's `metadata.xml` file has been generated, the service provider definition can be defined in the SAML identity provider.
For Red Hat SSO, a realm can be created for one or more appliances with individual clients defined one per appliance, where the client ID is specified as the URL of the appliance.
+
To add a client in the Red Hat SSO {product-title_short} realm:
+
.. Select and import the `miqsp-metadata.xml` file created for `mod_auth_mellon`.
.. Set the client ID as `https://<miq-appliance>`.
.. Set the client protocol as `saml`.
+
. Update the client definition for the appliance in Red Hat SSO with the following:
+
[options="header"]
|=========================================================================================
| Setting                                     | Value
| Name ID Format                              | username
| Valid Redirect URIs                         | https://<miq-appliance>/saml2/postResponse
| Assertion Consumer Service POST Binding URL | https://<miq-appliance>/saml2/postResponse
| Logout Service Redirect Binding URL         | https://<miq-appliance>/saml2/logout
|=========================================================================================
+
. Obtain the identity provider’s `idp-metadata.xml` metadata file as follows:
+
------
# cd /etc/httpd/saml2
# curl -s -o idp-metadata.xml \
  http://<redhatSSO-server>:8080/auth/realms/<miq-realm>/protocol/saml/descriptor
------
+
. In CloudForms 4.6-beta, the following change is necessary to the `idp-metadata.xml` file for SAML logout to work between `mod_auth_mellon` and Red Hat SSO:
+
------
# vi idp-metadata.xml

  ...
  <SingleLogoutService
<   Binding="urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST"
---
>   Binding="urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect"
    Location=
  ...
------
+
. Restart the HTTP server on the appliance:
+
------
# systemctl restart httpd
------


[[configure-appliance-UI-SAML]]
*Configuring the Appliance Administrative User Interface*

After configuring the HTTP server for SAML, update the {product-title_short} appliance so that the administrative user interface works with SAML authentication.

. Login to the appliance as `admin`, and navigate to the settings menu, then menu:Configuration[Authentication].
. Set the mode to *External (httpd)*.
. Check *Enable SAML*. This enables the SAML login button on the appliance login screen, then redirects to the SAML protected page for authentication, and supports the SAML logout process.
. Check *Enable Single Signon*. With this option enabled, initial access to the appliance's administrative user interface redirects to the SAML identity provider authentication screen. Logging out from the appliance returns the user to the appliance login screen, allowing them to log in as `admin` unless *Disable Local Login* is also checked.
. Optional: Check *Disable Local Login* to disable the `admin` login to appliance and only allow SAML based authentication. Note that if there are issues with the identity provider or you require `admin` access to the appliance, you cannot log in through the appliance login screen until you re-enable local login as described in xref:re-enable-local-login[].
. Check *Get User Groups from External Authentication (httpd)*
. Click *Save*.

[IMPORTANT]
======
Ensure the user’s groups are created on the appliance and appropriate roles are assigned to those groups. See _SAML Assertions_ in xref:saml-assertions[] for more information on the parameters used by the {product-title_short} appliance.

For example, to configure user groups from your SAML identity provider to work with {product-title_short}:

  . In your SAML identity provider, specify your existing user groups in similar format to the following: `REMOTE_USER_GROUPS=Administrators;CloudAdministrators;Users`
  . On your {product-title_short} appliance, create the equivalent groups. See _Creating a User Group_ in xref:creating-a-user-group[].
  . On your {product-title_short} appliance, assign EVM roles to the groups. See _Creating a Role_ in xref:creating-a-role[].
======

Complete the above steps on each appliance in the settings menu, then navigate to menu:Configuration[Access Control].

You can now log into your {product-title_short} appliance using your SAML credentials.

[[saml-assertions]]
*SAML Assertions*

To authenticate to the {product-title_short} appliance using SAML, the following remote user parameters are looked at by the appliance upon a successful login and redirect from the identity provider. These parameters are used by the appliance to obtain group authentication information.


[options="header",cols="<2,<1",width="70%"]
|==============================================
| HTTP Environment           | SAML Assertion
| REMOTE_USER                | username
| REMOTE_USER_EMAIL          | email
| REMOTE_USER_FIRSTNAME      | firstname
| REMOTE_USER_LASTNAME       | lastname
| REMOTE_USER_FULLNAME       | fullname
| REMOTE_USER_GROUPS         | groups
|==============================================

For Red Hat SSO, the above SAML assertions can be defined for the appliance client in Red Hat SSO as mappers.

[options="header",cols="<1,<2,<1,<1"]
|============================================================================
| Name       | Category                  | Type           | Property
| username   | AttributeStatement Mapper | User Property  | username
| email      | AttributeStatement Mapper | User Property  | email
| firstname  | AttributeStatement Mapper | User Property  | firstName
| lastname   | AttributeStatement Mapper | User Property  | lastName
| fullname   | AttributeStatement Mapper | User Attribute | fullName
| groups     | Group Mapper              | Group List     | groups
|============================================================================

[IMPORTANT]
======
The `fullName` attribute was not available in the default database as of this writing and was added as a user attribute.
======



[[re-enable-local-login]]
*Re-enabling Local Login (Optional)*

If you disabled local login in the administrative user interface but need the ability to log in as `admin`, local login can be re-enabled using one of the following methods:

.Re-enabling Local Login from the Appliance Administrative User Interface

This method requires the identity provider to be available, and the ability to login as a user with enough administrative privileges to update {product-title_short} authentication settings.

. Log in to the appliance user interface as the administrative user.
. From the settings menu, select menu:Configuration[Authentication].
. Uncheck *Disable Local Login*.
. Click *Save*.

.Re-enabling Local Login from the Appliance Console:

. Use SSH to log into the appliance as `root`.
. Run the `appliance_console` command.
. Select *Update External Authentication Options*.
. Select *Enable Local Login*.
. Apply the updates.

Alternatively, log into the appliance as root using SSH, and run the following command:

------
# appliance_console_cli --extauth-opts local_login_disabled=false
------

////

[[workers-1]]
===== Workers

Use the Workers page to specify the number of workers and amount of memory allowed to be used for each type.

[NOTE]
======
Only make these changes when directed to by Red Hat Support.
======

[[changing-settings-for-a-worker]]
====== Changing Settings for a Worker

To change the settings for a worker

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Zones*.
. Click the zone where the server is located.
. Click on the server.
. Click *Workers*.
. Go to the type of worker you have been directed to change.
. If applicable, change Count or Memory Threshold using the dropdown boxes.
. Click *Save*.

[[database]]
===== Database

Use the Database page to specify the location of your Virtual Machine Database (VMDB) and its login credentials. By default, the type is PostgreSQL on the Server.

[NOTE]
==========
The server may not start if the database settings are changed. Be sure to validate your new settings before restarting the server.
==========

[[changing-a-database-setting]]
====== Changing a Database Setting

To change a database setting:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Zones*.
. Click the zone where the server is located.
. Click on the server.
. Click the *Database* tab.
. In the *Database* area, select the *Type* of database. You can select from *External Database on another CFME appliance*, *External Postgres Database*, and *Internal Database on this CFME Appliance*.
* Use *Hostname* to specify the IP address or hostname of the external database server.
* Use *Database Name* to specify the name of your VMDB.
* Specify the *User Name* to connect to the VMDB.
* Use *Password* and *Verify Password* to specify the password for the user name.
. Click *Validate* to check the settings.
. Click *Save*.
. Click *OK* to the warning that the server will restart immediately after you save the changes.

During the restart, you are unable to access the server. When the restart is complete, the new database settings are in effect.

[[customization-and-logos]]
===== Customization and Logos

[[custom-logos]]
====== Custom Logos

Use *Custom Logos* to display your own logo in the corner of the {product-title_short} user interface and on the login screen. Use the procedures below to upload a custom logo to the user interface, and to customize the login background and login panel text on the user interface.

[NOTE]
====
* If you have upgraded from an earlier {product-title} version and your custom logo was already in use before migration, although your logo image file is still in place in `vmdb/public/upload` you may have to uncheck and recheck the option to *Use Custom Logo Image* to re-enable displaying your custom logo. See xref:uploading-a-custom-logo-to-the-console[] for the procedure on how to access the *Use Custom Logo Image* option, or if you want to upload another custom logo to the user interface and customize the login background image and login panel text.

* Additionally, ensure the option to use configuration settings for the tenant under *Access Control* is set to `Yes`; see xref:display-custom-settings[] for the procedure on how to set the configuration settings.
====


[[uploading-a-custom-logo-to-the-console]]
====== Uploading a Custom Logo to the User Interface

[NOTE]
====
Make sure the desired logo is accessible from the computer where you are running the {product-title_short} user interface. The file must be in portable network graphics (png) format with dimensions of 350 px x 70 px.
====

To upload a custom logo to the user interface:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Zones*.
. Click the zone where the {product-title} server is located.
. Click on the server.
. Click the *Custom Logos* tab.
image:2071.png[]
. In *Custom Logo Image (Shown on top right of all screens)*, click *Choose file* to go to the location where the logo file is located.
. Click *Upload*. The icon is displayed above the file name box, and an option is shown to use the logo.
. Check *Use Custom Logo Image* to add the logo to your user interface.
. Click *Save*.

[NOTE]
====
To enable displaying your custom logo, ensure the option to use configuration settings for the tenant under *Access Control* is set to `Yes`. See xref:display-custom-settings[] for the procedure on how to set the configuration settings.
====

[[customizing-the-login-background]]
====== Customizing the Login Background

[NOTE]
====
Make sure the background image that you want to use is accessible from the computer where you are running the user interface. The file must be in PNG format with dimensions of 1280 px x 1000 px.
====

To customize the login background:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Zones*.
. Click the zone where the server is located.
. Click on the server.
. Click the *Custom Logos* tab.
. In *Custom Login & 'About' Screen Background Image*, click *Choose file* to go to the location where the background image file is located.
+
image:custom-login-about-background-image.png[]
+
. Click *Upload*. The icon is displayed above the file name box, and an option is shown to use the logo.
. Check *Use Custom Login Background Image* to add the background image to the login screen of the user interface.
. Click *Save*.


[[customizing-the-login-panel-text]]
====== Customizing the Login Panel Text

To customize the login panel text:

. From the settings menu, select *Configuration*.
. Navigate to menu:Settings[Configuration].
. Click on the *Settings* accordion, then click *Zones*.
. Click the zone where the server is located.
. Click on the server.
. Click the *Custom Logos* tab.
. In *Custom Login Panel Text*, enter the text that you want to display on the login screen.
. Click *Use Custom Login Text* to switch it to `Yes`.
image:custom-login-panel-text.png[]
. Click *Save*.


[[display-custom-settings]]
====== Displaying the Custom Configuration Settings

To enable displaying your custom logo in the corner of the {product-title} user interface and on the login screen:

. From the settings menu, select *Configuration*.
. Click the *Access Control* accordion.
. Click *Tenants*, then click *My Company*.
. Click image:1847.png[](*Configuration*), then click image:1851.png[](*Edit this item*).
+
image:use-configuration-settings.png[]
+
. Click *Use Configuration Settings* to switch it to `Yes`.
. Click *Save*.

[[advanced-settings]]
===== Advanced Settings

You may be instructed by Red Hat to edit some configuration settings manually. This feature is available for a limited number of options and can only be used by users assigned the super administrator role. Changing
settings using this procedure may disable your {product-title} server.

[NOTE]
=========
Only make manual changes to your configuration files if directed to do so by Red Hat.
=========

[[editing-configuration-files-manually]]
====== Editing Configuration Files Manually

To edit configuration files manually:

. From the settings menu, click *Configuration*.
. Click on the *Settings* accordion, then click *Zones*.
. Click the zone where the server is located.
. Click on the server.
. Click the *Advanced* tab.
. Select the configuration file to edit from the *Configuration File to Edit* area.
. Make the required changes.
. Click *Save*.


[[configuration-parameters]]
====== Configuration Parameters

Table: authentication

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|amazon_key|If using Amazon for the authentication mode, specify your Amazon Key. This is the same as Amazon Access Key in Configuration-Operations-Server-Amazon Settings in the appliance console. Default: blank
|amazon_secret|If using Amazon for the authentication mode, specify your Amazon Secret. This is the same as Amazon Secret Key in Configuration-Operations-Server-Amazon Settings in the appliance console. Default: blank
|basedn|If using ldap for the authentication mode, specify your Base DN. This is the same as Base DN in Configuration-Operations- Server-LDAP Settings in the appliance console. Default: blank
|bind_dn|The user name to bind to the LDAP server. This user must have read access to all users and groups that will be used for {product-title} authentication and role assignment. This is the same as Bind DN in Configuration-Operations-Server-LDAP Settings in the appliance console. Default: blank
|bind_pwd:|The password for the bind_dn user. This is the same as Bind Password in Configuration-Operations- Server-LDAP Settings in the appliance console. Default: blank
|get_direct_groups|Use this to get the LDAP roles from the LDAP users' home forest. This is the same as Get Roles from Home Forest in the Authentication page for the {product-title} Server. Default: true
|group_memberships_max_depth|When traversing group memberships in the LDAP directory it will stop at this value. Default: 2
|ldaphost|Use ldaphost to specify the fully qualified domain name of your LDAP server. This is the same as LDAP Host Name in Configuration-Operations-Server-LDAP Settings in the appliance console. Default: blank
|ldapport|Specify the port of your LDAP server. This is the same as LDAP Port in Configuration-Operations- Server-LDAP Settings in the appliance console. Default: 389
|mode|Use database to use the VMDB for security. Use ldap or ldaps to use directory services. This is the same as Mode in Configuration-Operations-Server-Authentication in the appliance console. Default: database
|user_type|Use userprincipalname to type the user name in the format of user@domainname. Use mail to login with the user's e-mail address. Use dn-cn for Distinguished Name (CN=<user>) or dn-uid Distinguished Name (UID=<user>) to use just the user name, but be sure to enter the proper user_suffix for either one. This is the same as User Type in Configuration-Operations- Server-LDAP Settings in the appliance console. Default: userprincipalname
|user_suffix|Domain name to be used with user_type of dn-cn or dn-uid. This is the same as User Suffix in Configuration-Operations- Server-LDAP Settings in the appliance console. Default: blank
|=======================================================================


Table: coresident_miqproxy

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|use_vim_broker|Specify if you want the coresident SmartProxy to use a shared connection through the VIM broker to communicate with the VC or ESX host for SmartState Analysis. If it is disabled, then each SmartProxy SmartState Analysis would create its own connection. Default: true
|concurrent_per_ems|Specify the number of co-resident SmartProxy SmartState Analyses that can be run against a specific management system at the same time. Default: 1
|concurrent_per_host|Specify the number of co-resident SmartProxy SmartState Analyses that can be run against a specific host at the same time. Default: 1
|scan_via_host|If you change scan_via_host to false, {product-title} will use the Management System to scan which is limited by the concurrent_per_ems setting instead of the concurrent_per_host setting. Note this will greatly increase traffic to the Management System. Default: true
|=======================================================================

Table: ems_refresh

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|capture_vm_created_on_date|Set to false to turn off historical event retrieval. Set to true to turn on. By setting the flag to true {product-title} will try to set the "ems_created_on" column in the vms table after an ems refresh for new VMs and any VMs with a nil "ems_created_on" value. {product-title} looks at event information in our database as well as looking up historical event data from the management system. This is optional since the historical lookup could timeout. Default: false
|collect_advanced_settings|Set to false if you do not want to collect advanced Virtual Machine settings during a management system refresh. This will increase the speed of the refresh, but less data will be collected. If the parameter is not listed, then the value is true. Default: true
|ec2|
|get_private_images|For EC2 refreshes only; whether or not to retrieve private images. Default: true
|get_public_images|For EC2 refreshes only; whether or not to retrieve public images. Default: false. Warning: setting get_public_images to true loads several thousand images in the VMDB by default and may cause performance issues.
|get_shared_images|For EC2 refreshes only; whether or not to retrieve shared images. Default: true
|public_images_filters|For EC2 refreshes only; a filter to reduce the number of public images. Default: all public images
|ignore_terminated_instances|For EC2 refreshes only; whether or not to ignore terminated instances. Default: true
|full_refresh_threshold|The number of targeted refreshes requested before they are rolled into a full refresh. For example, if the system and/or the user target a refresh against 7 VMs and 2 Hosts (9 targets), when the refresh actually occurs it will do a partial refresh against those 9 targets only. However, if a 10th had been added, the system would perform a full EMS refresh instead. Default: 100
|raise_vm_snapshot_complete_if_created_within:|Raises vm_snapshot_complete event for a snapshot being added to VMDB only if the create time in Virtual Center is within the configured period of time. This prevents raising events for old snapshots when a new VC is added to {product-title}. Default: 15.minutes
|refresh_interval|Scheduler does a periodic full EMS refresh every refresh_interval. Default: 24.hours
|=======================================================================

Table: host_scan

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|queue_timeout|Time period after which a host SmartState analysis will be considered timed out. Default: 20.minutes
|=======================================================================


Table: log

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|level|Specify the required level of logging for the {product-title} appliance. Possible levels from most detailed to least detailed are: debug, info, warn, error, fatal. This is the same as Log Level in Configuration-Operations-Server-Logging in the appliance console and applies immediately to the evm.log file. Default: info
|level_aws|Specify the level of logging for Amazon Web Services communications. Possible levels from most detailed to least detailed are: debug, info, warn, error, fatal. This applies to the aws.log file. Default: info
|level_aws_in_evm|Specify what level of Amazon Web Services communication log should be also shown in evm.log. Possible levels from most detailed to least detailed are: debug, info, warn, error, fatal. Default: error
|level_fog|Specify the level of logging for Fog communications. Possible levels from most detailed to least detailed are: debug, info, warn, error, fatal. This applies to the fog.log file. Default: info
|level_fog_in_evm|Specify what level of Fog communication log should be also shown in evm.log. Possible levels from most detailed to least detailed are: debug, info, warn, error, fatal. Default: error
|level_rails|Specify the level of logging for Rails. Possible levels from most detailed to least detailed are: debug, info, warn, error, fatal. Once changed, this applies immediately to the production.log file. Default: info
|level_rhevm|Specify the level of logging for Red Hat communications. Possible levels from most detailed to least detailed are: debug, info, warn, error, fatal. This applies to the rhevm.log file. Default: warn
|level_rhevm_in_evm|Specify what level of Red Hat communication log should be also shown in evm.log. Possible levels from most detailed to least detailed are: debug, info, warn, error, fatal. Default: error
|level_vim|Specify the level of logging for VIM (communication with VMware ESX and Virtual Center). Possible levels from most detailed to least detailed are: debug, info, warn, error, fatal. This applies to the vim.log file. Default: warn
|level_vim_in_evm|Specify what level of vim logging should be also shown in evm.log. Possible levels from most detailed to least detailed are: debug, info, warn, error, fatal. Default: error
|=======================================================================


Table: db_stats

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|enabled|Specify if you want to keep track of the number of queries, size of queries, number of responses, size of response, min/max for each, number of established connections at for each server process. This information will show in the EVM log. Default: false
|log_frequency|How frequently in seconds the process will log the database statistic in seconds. Default: 60
|=======================================================================


Table 3.7. callsites

Table: log

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|enabled|Specify if you want keep track of the code that is accessing the database. Enabling call sites will decrease performance because of the amount of information tracked. The db_stats: enabled parameter must be set to true to use this. Default: false
|depth|Specify how many levels in the call stack to track for each database access. Default: 10
|min_threshold|Do not keep track of code that does not access the database this many times per log_frequency. Default: 10
|path|Set the path for the {product-title} appliance log. This is the same as Log Path in Configuration-Operations- Server-Logging in the appliance console. Default: If no value is present, the path is /var/www/miq/vmdb/log.
|line_limit|Limit how many characters are retained in a single log line. 0 means no limit. Default: 0
|=======================================================================


Table 3.8. collection

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|ping_depot|Whether to use TCP port ping to the log depot before performing log collection. Default: true
|ping_depot_timeout|Specify how long in seconds to wait for response from log depot before deciding that the TCP port ping failed. Default: 20
|current|When collecting logs, specifies what is considered current logging as opposed to archived logging. Default: :pattern:

log/\*.log

log/apache/*.log

log/\*.txt

config/*

/var/opt/rh/rh-postgresql95/lib/pgsql/data/\*.conf

/var/opt/rh/rh-postgresql95/lib/pgsql/data/pg_log/*

/var/log/syslog*

/var/log/daemon.log*

/etc/default/ntp*

/var/log/messages*

/var/log/cron*

BUILD

GUID

VERSION

|archive|Specifies what is considered archived logging. The default pattern is blank which means *.gz files in the log directory.
|=======================================================================


Table 3.9. log_depot

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|uri|Specify the URI for the log depot. This is the same as the URI in menu:Configuration[Diagnostics > Collect Logs] in the appliance console. Default: blank
|username|Specify the user name for the log depot. This is the same as the user ID in menu:Configuration[Diagnostics > Collect Logs] in the appliance console. Default: blank
|password|Specify the password for the user for the log depot. This is the same as the password in menu:Configuration[Diagnostics > Collect Logs] in the appliance console. Default: blank
|=======================================================================

Table: performance

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|capture_threshold|
|vm|Amount of time in minutes to wait after capture before capturing again. Default: 50.minutes
|host|Amount of time in minutes to wait after capture before capturing again. Default: 50.minutes
|ems_cluster|Amount of time in minutes to wait after capture before capturing again. Default: 50.minutes
|storage|Amount of time in minutes to wait after capture before capturing again. Default: 120.minutes
|capture_threshold_with_alerts|
|host|Amount of time in minutes to wait after capture before capturing again. This value is used instead of capture_threshold for Hosts that have alerts assigned based on real time Capacity & Utilization data. Default: 20.minutes
|ems_cluster|Amount of time in minutes to wait after capture before capturing again. This value is used instead of capture_threshold for clusters that have alerts assigned based on real time Capacity & Utilization data. Default: 50.minutes
|vm|Amount of time in minutes to wait after capture before capturing again. This value is used instead of capture_threshold for VMs that have alerts assigned based on real time Capacity & Utilization data. Default: 20.minutes
|concurrent_requests|
|vm|Amount of time in minutes to wait after capture before capturing again. This value is used instead of capture_threshold for VMs that have alerts assigned based on real time Capacity & Utilization data. Default: 20.minutes
|hourly|Number of concurrent VC requests to make when capturing hourly raw metrics. Default: 1
|realtime|Number of concurrent VC requests to make when capturing real time raw metrics. Default: 20
|history|
|initial_capture_days|How many days to collect data for on first collection. Default: 0
|Keep_daily_performances|How long to keep daily performance data in the VMDB. Default: 6.months
|keep_realtime_performances|How long to keep realtime performance data in the VMDB. Default: 4.hours
|keep_hourly_performances|How long to keep hourly performance data in the VMDB. Default: 6.months
|purge_window_size|When the purge needs to delete rows which are older than the keep_realtime_performances, keep_hourly_performances, and keep_daily_performances values, this value sets how many rows to delete in each batch. For example, a value of 1000 will cause us to issue ten 1,000 row deletes. Default: 1000
|=======================================================================

Table 3.11. repository_scanning

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|defaultsmartproxy|Specify the SmartProxy for repository scanning. This is the same as Default Repository Smartproxy in Configuration-Operations- Server-VM Server Control in the appliance console. Default: blank
|=======================================================================

Table 3.12. server

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|case_sensitive_name_search|Specify if you want the search by name on configuration item screens to be case senstive. Default: false
|company|Specify the label you want to use for your company's tagging. This is the same as Company Name in Configuration-Operations- Server-Basic Info. Default: "My Company"
|custom_logo|Specify if you want to use a custom logo. This is the same as Use Custom Logo in Configuration-Custom Logo-Logo Selection. Default: false
|events|
|disk_usage_gt_percent|For {product-title} operational alerts, specify at what threshold the disk usage alerts will be triggered. Default: 80
|heartbeat_timeout|How long to wait until the server heartbeat is considered timed out. if the timeout is exceeded, other appliances in the zone/region can vie for the roles active on the timed out {product-title} appliance. Default: 2.minutes
|host|{product-title} Server's IP address. Default: blank
|hostname|{product-title} Server's hostname. Default: localhost.localdomain
|listening_port|Specify the port number on which the web server is listening. Note that this does not set the port that VMDB listens on. When deploying the SmartHost from the {product-title} appliance, it tells the SmartHost (miqhost) what port to talk to the VMDB on. Default: "443"
|mks_version|Specify the version of the VMware MKS Plugin to use for the VM Console. This is the same as VMware MKS Plugin Version in Configuration-Operations- Server-VM Console. Default : 2.1.0.0
|name|Set the name to display for the {product-title} appliance that you are logged on to in the appliance console. This is the same as appliance Name in Configuration-Operations- Server-Basic Information. Default : EVM
|role|Specify the roles for this {product-title} Server, separated by commas without spaces. The possible values are automate, database_operations, ems_inventory, ems_metrics_collector, ems_metrics_coordinator, ems_metrics_processor, ems_operations, event, notifier, reporting, scheduler, smartproxy, smartstate, user_interface, web_services. This is the same as Server Roles in Configuration-Operations- Server- Server Control. Default: database_operations, event, reporting, scheduler, smartstate, ems_operations, ems_inventory, user_interface, web_services
|session_store|Where to store the session information for all web requests. The possible values are sql, memory, or cache. SQL stores the session information in the database regardless of the type of database server. Memory stores all the session information in memory of the server process. Cache stores the information in a memcache server. Default: cache
|startup_timeout|The amount of time in seconds that the server will wait and prevent logins during server startup before assuming the server has timed out starting and will redirect the user to the log page after login. Default: 300
|timezone|Set the timezone for the {product-title} appliance. Default: UTC
|vnc_port|If using VNC for remote console, the port used by VNC. Default: 5800
|zone|Set the Zone for this appliance belongs. This is the same as Zone in Configuration-Operations- Server-Basic Information. Default : default
|:worker_monitor|Starts and monitors the workers. Parameters specified here will override those set in the workers:default section.
|poll|How often the worker monitor checks for work. This value only is only used when the worker has no more work to do from the queue. It will wait for an amount of time determined by the poll value and poll method. Therefore, if there is constant work on the queue, the worker will not wait in between messages. Default: 15.seconds
|miq_server_time_threshold|How much time to give the server to heartbeat before worker monitor starts to take action against non-responding server. Default: 2.minutes
|nice_delta|Tells the worker monitor what Unix "nice" value to assign the workers when starting. A lower number is less nice to other processes. Default: 1
|sync_interval|Time interval to sync active roles and configuration for all workers. Default: 30.minutes
|wait_for_started_timeout|How long to wait for a started worker to heartbeat before considering the worker timed out. Default: 10.minutes
|kill_algorithm|
|name|Criteria used to start killing workers. Default: used_swap_percent_gt_value
|value|Value of the criteria used. Default: 80
|start_algorithm|
|name|After server startup, criteria that must be met to decide if the {product-title} Server can start a new worker. Default: used_swap_percent_lt_value
|value|Value of criteria used. Default: 60
|=======================================================================

Table: session

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|interval|Set the time interval in seconds for checking inactive sessions in appliance console. Default: 60
|timeout|Set the time period in seconds in which inactive console sessions are deleted. This is the same as Session Timeout in Configuration-Operations-Server-Authentication in the appliance console. Default: 3600
|memcache_server|If you choose memory for session_store, you need to specify the memcache_server to retrieve the session information from. Default: 127.0.1.1:11211
|memcache_server_opts|Options to send to memcache server. : blank
|show_login_info|Specify whether or not you want to see login info on start page. Default: true
|=======================================================================

Table: smartproxy_deploy

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|queue_timeout|Timeout for host smartproxy deploy job. Default: 30.minutes
|=======================================================================

Table 3.15. smtp

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|host|Specify the hostname of the smtp mail server. This is the same as Host in Configuration-Operations-Server-Outgoing SMTP E-mail Server. Default: localhost
|port|Specify the port of the smtp mail server. This is the same as Port in Configuration-Operations-Server-Outgoing SMTP E-mail Server. Default: "25"
|domain|Specify the domain of the smtp mail server. This is the same as Domain in Configuration-Operations-Server-Outgoing SMTP E-mail Server. Default: mydomain.com
|authentication|Specify the type of authentication of the smtp mail server. This is the same as Authentication in Configuration-Operations-Server-Outgoing SMTP E-mail Server. Default: login
|user_name|Specify the username required for login to the smtp mail server. This is the same as User Name in Configuration-Operations-Server-Outgoing SMTP E-mail Server. Default: evmadmin
|password|Specify the encrypted password for the user_name account. This is the same as Password in Configuration-Operations-Server-Outgoing SMTP E-mail Server. Default: blank
|from|Set the address that you want to send e-mails from. This is the same as From E-mail Address in Configuration-Operations-Server-Outgoing SMTP E-mail Server. Default: cfadmin@cfserver.com
|=======================================================================


Table 3.16. snapshots

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|create_free_percent|Ensures the % of free space available on the main datastore (datastore where vmx file is located) can support the % growth of the snapshot. The default is to require space for 100% of the provisioned size of all disks that are taking part in the snapshot. A value of 0 means do not check for space before creating the snapshot. Default: 100
|remove_free_percent|Ensures the % of free space available on the main datastore (datastore where vmx file is located) has the % free space available to support the snapshot deletion process. Note that the deletion process consists of first composing a new snapshot then removing it once the original snapshot to be deleted has been collapsed in the VM. The default is to require 100% of the size of all disks to complete this process. A value of 0 means do not check for space before removing the snapshot. Default: 100
|=======================================================================


Table 3.17. webservices

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|contactwith|Set to ipaddress to contact miqhost using the IP address. Set to hostname to contact miqhost by its hostname. Set to resolved_ipaddress to take the hostname and resolve it to an IP address. Default: ipaddress
|mode|Set to invoke to use webservices. Set to disable to turn off webservices. This is the same as Mode in Configuration-Operations- Server-Web Services in the appliance console. Default: invoke
|nameresolution|If set to true, the hostname will be resolved to an IP address and saved with the host information in the VMDB. Default: false
|security|If Web Services are enabled, you can set this to ws-security. This is the same as Security in Configuration-Operations- Server-Web Services in the appliance console. Note: This is not currently supported. Default: none
|timeout|Specify the web service timeout in seconds. Default: 120
|password|Specify the encrypted password for the user_name account. This is the same as Password in Configuration-Operations-Server-Outgoing SMTP E-mail Server. Default: blank
|use_vim_broker|Controls if the vim_broker is used to communicate with VMware infrastructure. Default: true
|=======================================================================


Table: workers

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Parameters|Description
|worker_base|
|defaults|If the following parameters are NOT explicitly defined for a specific worker, then these values will be used.
|count|Number of this type of worker. Default: 1
|gc_interval|How often to do garbage collection for this worker. Default: 15.minutes
|poll|How often the workers checks for work. This value only is only used when the worker has no more work to do from the queue. It will wait for an amount of time determined by the poll value and poll method. Therefore, if there is constant work on the queue, the worker will not wait in between messages. Default: 3.seconds
|poll_method|If set to normal, the worker checks for work the number of seconds set in the poll parameter. If set to escalate, the worker will increase the time between checks when there is no work to be done. Default: normal
|poll_escalate_max|The maximum number of time to wait between checks for work. Poll_method must be set to escalate for this option to be used. Default: 30.seconds
|heartbeat_freq|How often to "heartbeat" the worker. Default: 60.seconds
|heartbeat_method|Set which way to dispatch work. Possible values are sql or drb. Default: drb
|heartbeat_timeout|How long to wait until the worker heartbeat is considered timed out. Default: 2.minutes
|parent_time_threshold|How long to allow the parent to go without heartbeating before considering the "parent' not responding. For workers, the worker monitor is the parent. For Worker monitor, the Server is the parent. Default: 3.minutes
|memory_threshold|How much memory to allow the worker to grow to before gracefully requesting it to exit and restart. Default: 150.megabytes
|nice_delta|Tells the worker monitor what Unix "nice" value to assign the workers when starting. A lower number is less nice to other processes. Default: 10
|restart_interval|How long to let a worker remain up before asking it to restart. All queue based workers are set to 2.hours and every other worker does not get restarted by a 0.hours value. Default: 0.hours
|starting_timeout|How long to wait before checking a worker's heartbeat when it is starting up to mark it as not responding, similar to a grace period before you begin monitoring it. Default: 10.minutes
|event_catcher|Associated with Event Monitor Server Role. Captures ems events and queues them up for the event_handler to process. Parameters specified here will override those set in the worker_base:default section.
|ems_event_page_size|Internal system setting which sets the maximum page size for the event collector history. This should not be modified. Default: 100
|ems_event_thread_shutdown_timeout|Internal system setting which determines how long the event catcher at shutdown will wait for the event monitor thread to stop. This should not be modified. Default: 10.seconds
|memory_threshold|How much memory to allow the worker to grow to before gracefully requesting it to exit and restart. Default: 2.gigabytes
|nice_delta|Tells the worker monitor what Unix "nice" value to assign the workers when starting. A lower number is less nice to other processes. Default: 1
|poll|How often the workers checks for work. This value only is only used when the worker has no more work to do from the queue. It will wait for an amount of time determined by the poll value and poll method. Therefore, if there is constant work on the queue, the worker will not wait in between messages. Default: 1.seconds
|event_catcher_redhat|Contains settings that supersede the event_catcher for event_catcher_redhat.
|event_catcher_vmware|Contains settings that supersede the event_catcher for event_catcher_vmware.
|poll|How often the workers checks for work. This value only is only used when the worker has no more work to do from the queue. It will wait for an amount of time determined by the poll value and poll method. Therefore, if there is constant work on the queue, the worker will not wait in between messages. Default: 1.seconds
|event_catcher_openstack|Contains settings that supersede the event_catcher for event_catcher_openstack.
|poll|How often the workers checks for work. This value only is only used when the worker has no more work to do from the queue. It will wait for an amount of time determined by the poll value and poll method. Therefore, if there is constant work on the queue, the worker will not wait in between messages. Default: 15.seconds
|topics|List of AMQP topics that should be monitored by {product-title} when gathering events from OpenStack.
|duration|Qpid Specific. Length of time (in seconds) the receiver should wait for a message from the Qpid broker before timing out. Default: 10.seconds
|capacity|Qpid Specific. The total number of messages that can be held locally by the Qpid client before it needs to fetch more messages from the broker. Default: 50.seconds
|amqp_port|Port used for AMQP. Default: 5672
|schedule_worker|Settings for Scheduler Server Role and any other work that runs on a schedule. Parameters specified here will override those set in the worker_base:default section.
|db_diagnostics_interval|How frequently to collect database diagnostics statistics. Default: 30.minutes
|job_proxy_dispatcher_interval|How often to check for available SmartProxies for SmartState Analysis jobs. Default: 15.seconds
|job_proxy_dispatcher_stale_message_check_interval|How often to check for the dispatch message in the queue Default: 60.seconds
|job_proxy_dispatcher_stale_message_timeout|Kill a message if this value is reached. Default: 2.minutes
|job_timeout_interval|How often to check to see if a job has timed out. Default: 60.seconds
|license_check_interval|How often to check for valid license. Default: 1.days
|memory_threshold|How much memory to allow the worker to grow to before gracefully requesting it to exit and restart. Default: 150.megabytes
|nice_delta|Tells the worker monitor what Unix "nice" value to assign the workers when starting. A lower number is less nice to other processes. Default: 3
|performance_collection_interval|Controls how often the schedule worker will put performance collection request on the queue to be picked up by the collection worker. Default: 3.minutes
|performance_collection_start_delay|How long after {product-title} Server has started before starting capacity and utilization collection, if collection needs to be done. Default: 5.minutes
|poll|How often the workers checks for work. This value only is only used when the worker has no more work to do from the queue. It will wait for an amount of time determined by the poll value and poll method. Therefore, if there is constant work on the queue, the worker will not wait in between messages. Default: 15.seconds
|server_logs_stats_interval|How often to log the {product-title} Server statistics. Default: 5.minutes
|server_stats_interval|How often to collect the {product-title} Server statistics. Default: 60.seconds
|session_timeout_interval|How often to check to see if a UI (appliance console) session has timed out. Default: 30.seconds
|storage_file_collection_interval|How often to perform file inventory of storage locations. Default: 1.days
|storage_file_collection_time_utc|What time to perform file inventory of storage locations. Default: "06:00"
|vdi_refresh_interval|How often to refresh vdi inventory. Default: 20.minutes
|vm_retired_interval|How often to check for virtual machines that should be retired. Default: 10.minutes
|vm_scan_interval|How often to check virtual machines to see if scan needs to be done. Default: 10.minutes
|smis_refresh_worker|Settings for Storage Inventory Server Role and any other work that runs on a schedule. Parameters specified here will override those set in the worker_base:default section
|poll|How often the workers checks for work. This value only is only used when the worker has no more work to do from the queue. It will wait for an amount of time determined by the poll value and poll method. Therefore, if there is constant work on the queue, the worker will not wait in between messages. Default: 15.seconds
|connection_pool_size|Maximum number of database connections allowed per process. Default: 5
|memory_threshold|How much memory to allow the worker to grow to before gracefully requesting it to exit and restart. Default: 1.gigabytes
|nice_delta|Tells the worker monitor what Unix "nice" value to assign the workers when starting. A lower number is less nice to other processes. Default: 3
|smis_update_period|How frequently to update smis information. Default: 1.hours
|status_update_period|How frequently to update smis status. Default: 5.minutes
|stats_update_period|How frequently to update smis statistics. Default: 10.minutes
|vim_broker_worker|Launched for any of these roles: Capacity & Utilization Collector, SmartProxy, SmartState Analysis, Management System Operations, Management System Inventory. Also launched if the use_vim_broker setting is on. Provides connection pooling, caching of data to and from the VMware infrastructure. Parameters specified here will override those set in the workers:default section.
|heartbeat_freq|How often to heartbeat the worker. Default: 15.seconds
|memory_threshold|How much memory to allow the worker to grow to before gracefully requesting it to exit and restart. Default: 1.gigabytes
|nice_delta|Tells the worker monitor what Unix "nice" value to assign the workers when starting. A lower number is less nice to other processes. Default: 3
|poll|How often the workers checks for work. This value only is only used when the worker has no more work to do from the queue. It will wait for an amount of time determined by the poll value and poll method. Therefore, if there is constant work on the queue, the worker will not wait in between messages. Default: 1.seconds
|reconnect_retry_interval|Period after which connection is retried. Default: 5.minutes
|vim_broker_status_interval|Internal system setting which configures how much time to wait after receiving event updates before checking for more updates. Default: 0.seconds
|wait_for_started_timeout|Time between the worker's preload and startup time before considering the worker timed out. Default: 10.minutes
|ui_worker:|Settings for User Interface Server Role. Parameters specified here will override those set in the worker_base:default section.
|connection_pool_size|Maximum number of database connections allowed per process. Default: 5
|memory_threshold|How much memory to allow the worker to grow to before gracefully requesting it to exit and restart. Default: 1.gigabytes
|nice_delta: 1|Tells the worker monitor what Unix "nice" value to assign the workers when starting. A lower number is less nice to other processes. Default: 1
|poll|How often the workers checks for work. This value only is only used when the worker has no more work to do from the queue. It will wait for an amount of time determined by the poll value and poll method. Therefore, if there is constant work on the queue, the worker will not wait in between messages. Default: 60.seconds
|web_service_worker|Settings for Web Services Server Role. Parameters specified here will override those set in the worker_base:default section.
|connection_pool_size|Maximum number of database connections allowed per process. Default: 5
|memory_threshold|How much memory to allow the worker to grow to before gracefully requesting it to exit and restart. Default: 1.gigabytes
|nice_delta|Tells the worker monitor what Unix "nice" value to assign the workers when starting. A lower number is less nice to other processes. Default: 1
|poll|How often the workers checks for work. This value only is only used when the worker has no more work to do from the queue. It will wait for an amount of time determined by the poll value and poll method. Therefore, if there is constant work on the queue, the worker will not wait in between messages. Default: 60.seconds
|queue_worker_base|Base class of all queue workers that work off of the queue..
|defaults|If the following parameters are NOT explicitly defined for a queue worker, then these values will be used.
|cpu_usage_threshold|How much cpu to allow the worker to grow to before gracefully requesting it to exit and restart. Default: 100.percent
|queue_timeout|How long a queue message can be worked on before it is considered timed out. Default: 10.minutes
|memory_threshold|How much memory to allow the worker to grow to before gracefully requesting it to exit and restart. Default: 400.megabytes
|restart_interval|Queue workers restart interval. Default: 2.hours
|poll_method|If set to normal, the worker checks for work the number of seconds set in the poll parameter. If set to escalate, the worker will increase the time between checks when there is no work to be done. Default: normal
|generic_worker|Performs work that is not classified as any specific type of work. Processes all normal priority or non-specific queue items. Parameters specified here will override those set in the queue_worker_base:default section
|count|Number of this type of worker. Default: 4
|ems_refresh_worker|Performs all ems (management system) refreshes to keep the vmdb in sync with the state of the components of the virtual infrastructure in the various management systems. Parameters specified here will override those set in the queue_worker_base:default section
|poll|How often the workers checks for work. This value only is only used when the worker has no more work to do from the queue. It will wait for an amount of time determined by the poll value and poll method. Therefore, if there is constant work on the queue, the worker will not wait in between messages. Default: 10.seconds
|memory_threshold|How much memory to allow the worker to grow to before gracefully requesting it to exit and restart. Default: 2.gigabytes
|nice_delta|Tells the worker monitor what Unix "nice" value to assign the workers when starting. A lower number is less nice to other processes. Default: 7
|restart_interval|Queue workers restart interval. Default: 2.hours
|queue_timeout|How long a message can be worked on before it is considered timed out. Default: 120.minutes
|event_handler|Associated with Event Monitor Server Role. Handles all events caught by the event catcher worker. Parameters specified here will override those set in the workers:default section. Parameters specified here will override those set in the queue_worker_base:default section
|cpu_usage_threshold|How much cpu to allow the worker to grow to before gracefully requesting it to exit and restart. The value of 0 means that this worker will never be killed due to CPU usage. Default: 0.percent
|nice_delta|Tells the worker monitor what Unix "nice" value to assign the workers when starting. A lower number is less nice to other processes. Default: 7
|perf_collector_worker|Connects to VC/ESX to collect the raw performance data. Same as the Capacity & Utilization Data Collector Server Role. Parameters specified here will override those set in the queue_worker_base:default section count. Number of this type of worker. Default: 2
|count|Number of this type of worker. Default: 2
|poll_method|If set to normal, the worker checks for work the number of seconds set in the poll parameter. If set to escalate, the worker will increase the time between checks when there is no work to be done. Default: escalate
|nice_delta|Tells the worker monitor what Unix "nice" value to assign the workers when starting. A lower number is less nice to other processes. Default: 3
|perf_processor_worker|Processes the raw performance data into a reportable format. Same as the Capacity & Utilization Data Processor Server Role. Parameters specified here will override those set in the queue_worker_base:default section
|count|Number of this type of worker. Default: 2
|poll_method|If set to normal, the worker checks for work the number of seconds set in the poll parameter. If set to escalate, the worker will increase the time between checks when there is no work to be done. Default: escalate
|memory_threshold|How much memory to allow the worker to grow to before gracefully requesting it to exit and restart. Default: 400.megabytes
|nice_delta|Tells the worker monitor what Unix "nice" value to assign the workers when starting. A lower number is less nice to other processes. Default: 7
|priority_worker|Performs all high priority queue items including many tasks on behalf of the UI. UI requests are normally executed by a priority worker so they will not to block the UI. Parameters specified here will override those set in the queue_worker_base:default section
|count|Number of this type of worker. Default: 2
|memory_threshold|How much memory to allow the worker to grow to before gracefully requesting it to exit and restart. Default: 200.megabytes
|nice_delta|Tells the worker monitor what Unix "nice" value to assign the workers when starting. A lower number is less nice to other processes. Default: 1
|poll|How often the workers checks for work. This value only is only used when the worker has no more work to do from the queue. It will wait for an amount of time determined by the poll value and poll method. Therefore, if there is constant work on the queue, the worker will not wait in between messages. Default: 1.seconds
|reporting_worker|Compiles reports. Settings for Reporting Server Role. Parameters specified here will override those set in the queue_worker_base:default section
|count|Number of this type of worker. Default: 2
|nice_delta|Tells the worker monitor what Unix "nice" value to assign the workers when starting. A lower number is less nice to other processes. Default: 7
|smart_proxy_worker|Performs the embedded scanning of virtual machines. Settings for SmartProxy Server Role. Parameters specified here will override those set in the queue_worker_base:default section
|count|Number of this type of worker. Default: 3
|memory_threshold|How much memory to allow the worker to grow to before gracefully requesting it to exit and restart. Default: 600.megabytes
|queue_timeout|How long a queue message can be worked on before it is considered timed out. Default: 20.minutes
|restart_interval|Queue workers restart interval. Default: 2.hours
|=======================================================================

[[schedules]]
==== Schedules

[[scheduling-smartstate-analyses-and-backups]]
===== Scheduling SmartState Analyses and Backups

From the *Schedules* area in *Settings*, you can schedule the analyses of virtual machines, hosts, clusters, and datastores to keep the information current. Depending on which resource you want to analyze, you can filter which ones to analyze. You may also specify only one virtual machine or perform an analysis on all virtual machines. In addition, you can schedule compliance checks, and database backups.

[[scheduling-a-smartstate-analysis-or-compliance-check]]
====== Scheduling a SmartState Analysis or Compliance Check

To schedule a SmartState Analysis or Compliance Check:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Schedules*.
. Click image:1847.png[](*Configuration*), and image:plus_green.png[](*Add a new Schedule*).
. In the *Basic Information* area, type in a *Name* and *Description* for the schedule.
. Select *Active* to enable this scan.
. From the *Action* list, select the type of analysis to schedule. Based on the type of analysis you choose, you are presented with one of the following group boxes:
image:2079.png[]
* *VM Analysis*: Displays *VM Selection* where you can choose to analyze *All VMs*, *All VMs for Provider*, *All VMs for Cluster*, *All VMs for Host*, *A single VM*, or *Global Filters*.
* *Template Analysis*: Displays *Template Selection* where you can choose to analyze *All Templates*, *All Templates for Provider*, *All Templates for Cluster*, *All Templates for Host*, *A single Template*, or *Global Filters*.
* *Host Analysis*: Displays *Host Selection* where you can choose to analyze *All Hosts*, *All Hosts for Provider*, *A single Host*, or *Global Filters*.
+
[NOTE]
======
You can only schedule host analyses for connected virtual machines, not repository virtual machines that were discovered through that host.
Since repository virtual machines do not retain a relationship with the host that discovered them, there is no current way to scan them through the scheduling feature.
The host is shown because it may have connected virtual machines in the future when the schedule is set to run.
======
+
* *Cluster / Deployment Role Analysis*: Displays *Cluster Selection* where you can choose to analyze *All Clusters*, *All Clusters for Provider*, or *A single Cluster*.
* *Datastore Analysis*: Displays *Datastore Selection* where you can choose to analyze *All Datastores*, *All Datastores for Host*, *All Datastores for Provider*, *A single Datastore*, or *Global Filters*.
* *VM Compliance Check*: Displays *VM Selection* where you can choose to analyze *All VMs*, *All VMs for Provider*, *All VMs for Cluster*, *All VMs for Host*, *A single VM*, or *Global Filters*.
* *Host Compliance Check*: Displays *Host Selection* where you can choose to analyze *All Hosts*, *All Hosts for Provider*, *All Hosts for Cluster*, *A single Host*, or *Global Filters*.
* *Database Backup*: Under *Type*, displays *Network File System* and *Samba*. See xref:database_backup[] for details on scheduling a database backup.
. By applying *Global Filters* within any of the above items, you can designate which virtual machines or hosts to analyze.
. In *Run*, set the frequency of the analysis to run. There are further options based on which *Run* option you choose.
* Click *Once* to have the analysis run only one time.
* Click *Daily* to run the analysis on a daily basis. You will be prompted to select the number of days between each analysis.
* Click *Hourly* to run the analysis hourly. You will be prompted to select the number of hours between each analysis.
. Select a *Time Zone*.
+
[NOTE]
======
If you change the *Time Zone*, you will need to reset the stating date and time.
======
+
. Type or select a date to begin the schedule in *Starting Date*.
. Select a *Starting Time* based on a 24 hour clock in the selected Time Zone.
. Click *Add*.

[[database_backup]]
===== Scheduling a Database Backup

To schedule a database backup:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Schedules*.
. Click image:1847.png[] (*Configuration*), and image:plus_green.png[](*Add a new Schedule*).
. In the *Basic Information* area, type in a *Name* and *Description* for the schedule.
image:2082.png[]
. Select *Active* to enable this backup schedule.
. From the *Action* list, select *Database backup*.
. In the *Database Backup Settings* area, select a type of server to put the backups. You can either use *Network File System* or *Samba*.
* If selecting *Samba*, enter the *Depot Name*, *URI*, *User ID*, and a valid *Password*. Then, click *Validate* to check the settings.
* If you choose *Network File System*, enter the *Depot Name* and *URI*.
. In *Run*, set the frequency of the analysis to run. There are further options based on which *Run* option you choose.
* Click *Once* to have the backup run only one time.
* Click *Daily* to run the backup on a daily basis. You will be prompted to select the number of days between each backup.
* Click *Hourly* to run the backup hourly. You will be prompted to select the number of hours between each backup.
. Select a *Time Zone*.
+
[NOTE]
======
If you change the *Time Zone*, you will need to reset the stating date and time.
======
+
. Type or select a date to begin the schedule in *Starting Date*.
. Select a *Starting Time* (UTC) based on a 24 hour clock in the selected time zone.
. Click *Add*.

[[modifying-a-schedule]]
====== Modifying a Schedule

To modify a schedule:

. From the settings menu, select *Configuration*.
. Click on the *Settings* accordion, then click *Schedules*.
. Click the schedule that you want to edit.
. Click image:1847.png[] (*Configuration*), and then click image:1851.png[] (*Edit this Schedule*).
. Make the required changes.
. Click *Save*.

[[access-control]]
=== Access Control

From the settings menu, select *Configuration*. Click on the *Access Control* accordion to see a hierarchy of the configurable items for users, groups, roles, and tenants. You can add and modify users, groups, account roles, tenants, and projects.

ifdef::cfme[]
[NOTE]
====
For information about tenancy in {product-title_short}, and the difference between a tenant and project, see link:https://access.redhat.com/documentation/en-us/red_hat_cloudforms/4.6-beta/html-single/deployment_planning_guide/#tenants[Tenancy] in the _Deployment Planning Guide_.
====
endif::cfme[]

[[creating-a-tenant]]
==== Creating a Tenant

To create a tenant:

. From the settings menu, select *Configuration*.
. Click on the *Access Control* accordion, then click *Tenants*.
. Click on the top-level *Tenant*, click image:1847.png[] (*Configuration*), and image:plus_green.png[] (*Add child Tenant to this Tenant*) to create a tenant.
. Enter a name for the tenant in the *Name* field.
. Enter a description for the tenant in the *Description* field.
. Click *Add*.

[[creating-a-project]]
==== Creating a Project

To create a project:

. From the settings menu, select *Configuration*.
. Click on the *Access Control* accordion, then click *Tenants*.
. Click on the *Tenant* where you want to add a *Project*, click image:1847.png[] (*Configuration*), and image:plus_green.png[] (*Add Project to this Tenant*) to create a project.
. Enter a name for the project in the *Name* field.
. Enter a description for the project in the *Description* field.
. Click *Add*.

[[creating-a-project-quota]]
==== Managing Tenant and Project Quotas

Use the following procedure to allocate or edit quotas for tenants and projects.

. From the settings menu, select *Configuration*.
. Click on the *Access Control* accordion, then click *Tenants*.
. Click on the *Tenant* or *Project* where you want to add a quota, click image:1847.png[] (*Configuration*), and image:1851.png[] (*Manage quotas for the Selected Item*) to create a quota.
. In the list of pre-built quotas, switch *Enforced* next to the quota item you want to enable to `Yes`.
. In the *Value* field, enter the constraints you want to apply to the quota.
image:manage-quotas.png[]
. Click *Save*.


[[tagging-tenants-and-projects]]
==== Tagging Tenants and Projects
To tag tenants and projects:

. From the settings menu, select *Configuration*.
. Click on the *Access Control* accordion, then click *Tenants*.
. Select the tenant or project, then click *Policy*, and select *Edit My Company Tags for this Tenant*.
. In *Tag Assignment*, click *Select a customer tag to assign*, and select a tag from the list. In the next column, set a corresponding value.
. Click *Save*.


[[creating_a_user]]
==== Creating a User

To create a user:

. From the settings menu, select *Configuration*.
. Click on the *Access Control* accordion, then click *Users*.
. Click image:1847.png[] (*Configuration*), and image:plus_green.png[] (*Add a new User*) to create a user.
. Enter a *Full Name*, *Username*, *Password* with confirmation, and *Email Address* for the user.
+
image:available_groups.png[]
+
[IMPORTANT]
====
* If you are using LDAP, but did not enable *Get User Groups from LDAP* in your server's *Authentication* tab, you will need to define a user. The UserID must match exactly the user's name as defined in your directory service. Use all lowercase characters to ensure the user can be found in the VMDB.
* When the user logs in, they use their LDAP password.
ifdef::cfme[]
* For more information on configuring LDAP authentication in {product-title_short}, see https://access.redhat.com/documentation/en-us/red_hat_cloudforms/4.6-beta/html-single/managing_authentication_for_cloudforms/#ldap_config[Configuring LDAP or LDAPS Authentication] in _Managing Authentication_.
endif::cfme[]
====
+
. Select one or more groups from *Available Groups*.
. Click *Add*.

[[deleting-a-user]]
==== Deleting a User

For security reasons, delete any user that no longer needs access to the information or functions of the server.

To delete a user:

. From the settings menu, select *Configuration*.
. Click on the *Access Control* accordion, then click *Users*.
. Select the user you want to delete.
. Click image:1847.png[] (*Configuration*), and image:gui_delete.png[] (*Delete selected Users*) to delete a user.

[[groups]]
==== Groups

User groups create filters and assign roles to users. You can either create your own  groups, or leverage your LDAP directory service to assign groups of users to account roles.
For a list of what each pre-defined account role can do, see xref:roles[].

A user can exist in multiple groups. However, a group can only be assigned one account role.

ifdef::cfme[]
[NOTE]
====
See https://access.redhat.com/documentation/en-us/red_hat_cloudforms/4.6-beta/html-single/managing_authentication_for_cloudforms/#assigning_account_roles_using_ldap_groups[Assigning CloudForms Account Roles Using LDAP Groups] in _Managing Authentication_.
====
endif::cfme[]

[[creating-a-user-group]]
==== Creating a Group

To create a user group:

. From the settings menu, select *Configuration*.
. Click on the *Access Control* accordion, then click *Groups*.
. Click image:1847.png[] (*Configuration*), and image:plus_green.png[] (*Add a new Group*) to create a group.
. Enter a name for the group in the *Name* field. To ensure compatibility with tags, use underscores in place of spaces. For example, {product-title}-`test_group`.
. Select a *Role* to map to this group. For a description of each {product-title_short} role, see xref:account-roles-and-descriptions[].
. Select the *Project/Tenant* this group must belong to.
. Limit what users in this group can view by selecting filters in the *Assign Filters* area. 
.. Click the *<My Company> Tags* tab to select the tags that users in this group can access. Resources with the selected tags attached can be accessed by this group. Select tags using one of the options in the *This user is limited to* list:
* Select *Specific Tags*, then check the boxes for the tags that you want to limit this user to. The items that have changed will show in blue italicized font.
* Select *Tags Based On Expression*, then create tags based on an expression using AND, OR, or NOT. This allows you to further limit the resources accessible to a user: for example, to specify a combination of tags that must exist on a resource.
image:tag_expression_editor.png[]
.. Click the *Host & Clusters* tab.
... Check the boxes for the host and clusters that you want to limit this user to. The items that have changed will show in blue italicized font.
image:2088.png[]
.. Click the *VMs & Templates* tab. This shows folders that you have created in your virtual infrastructure.
... Check the boxes for the folders that you want to limit this user to. The items that have changed will show in blue italicized font.
. Click *Add*.

After creating a group, assign one or more users to the group by editing a user.

[[roles]]
==== Roles

When you create a group, you must specify a role to give the group rights to resources in the console. The group's role determines the scope of access for the users that are members of the group.

image:CloudForms_General_Config_Roles_460469_1017_JCS.png[]

A group can only be assigned one role when the local database is used for authentication, as shown in the above image.

{product-title} provides a default group of roles, but you can also create your own, or copy and edit the default groups.

[NOTE]
====
If you have enabled *Get Role from LDAP* under *LDAP Settings*, then the role is determined by the LDAP user's group membership in the directory service. 
ifdef::cfme[]
See https://access.redhat.com/documentation/en-us/red_hat_cloudforms/4.6-beta/html-single/managing_authentication_for_cloudforms/#ldap_config[Configuring LDAP or LDAPS Authentication] in _Managing Authentication_.
endif::cfme[]
====

To view details of a role and its level of access:

. From the settings menu, select *Configuration*.
. Click on the *Access Control* accordion, then click *Roles*.
. Click on a role from the list to display role information and the product features the role can access (marked by a checkmark). You can expand the categories under *Product Features* to see further detail.

The table below shows a summary of the functions available to each role.

[[account-roles-and-descriptions]]
===== Account Roles and Descriptions

[width="100%",cols="30%,70%",options="header",]
|=======================================================================
|Role|Description
|Administrator|Administrator of the virtual infrastructure. Can access all infrastructure functionality. Cannot change server configuration.
|Approver|Approver of processes, but not operations. Can view items in the virtual infrastructure, view all aspects of policies and assign policies to policy profiles. Cannot perform actions on infrastructure items.
|Auditor|Able to see virtual infrastructure for auditing purposes. Can view all infrastructure items. Cannot perform actions on them.
|Container Administrator|Administrator with capabilities to configure, view and execute tasks on all containers and related underlying infrastructure. Has access to Nodes, Pods and Projects dashboards.
|Container Operator|This role can view and execute tasks related to containers and related underlying infrastructure. The Container Operator has access to locked versions of the same dashboards as the Container Administrator.
|Desktop|Access to VDI pages.
|Operator|Performs operations of virtual infrastructure. Can view and perform all functions on virtual infrastructure items including starting and stopping virtual machines. Cannot assign policy, but can view policy simulation from Virtual Machine page.
|Security|Enforces security for the virtual environment. Can assign policies to policy profiles, control user accounts, and view all parts of virtual infrastructure. Cannot create policies or perform actions on virtual infrastructure.
|Super Administrator|Administrator of {product-title} and the virtual infrastructure. Can access all functionality and configuration areas.
|Support|Access to features required by a support department such as diagnostics (logs). Can view all infrastructure items and logs. Cannot perform actions on them.
|Tenant Administrator|Configures settings applicable to a Tenant. Sets Branding, maps groups/roles, configures LDAP credentials, and configures dashboard settings.
|Tenant Quota Administrator|Configures quota limits for the tenant, applying usage constraints for CPU, Memory, Storage, Maximum number of VMs, and Maximum number of Templates.
|User|User of the virtual infrastructure. Can view all virtual infrastructure items. Cannot perform actions on them.
|User Limited Self Service|Limited User of virtual machines. Can make provision requests. Can access some functions on the virtual machine that the user owns including changing power state.
|User Self Service|User of virtual machines. Can make provision requests. Can access some functions on the virtual machine that the user owns and that the user's LDAP groups own including changing power state.
|Vm User|User of virtual machines. Can access all functions on the virtual machine including changing power state and viewing its console. Cannot assign policy, but can view policy simulation from virtual machine page.
|=======================================================================

[[creating-a-role]]
==== Creating a Role

To create a role:

. From the settings menu, select *Configuration*.
. Click on the *Access Control* accordion, then click *Roles*.
. Click image:1847.png[] (Configuration), and image:plus_green.png[] (Add a new Role). Alternatively, you can copy an existing role to a new role by clicking *Copy this to a new Role*.
. In the *Role Information* area, type a name for the new role. For *VM & Template Access Restriction*, select if you want to limit users with this role to only see virtual machines owned by the user, owned by the user or its group, or all virtual machines.
image:2095.png[]
. Under *Product Features (Editing)*, navigate to the appropriate feature and enable or disable it.
image:2096.png[]
. Click *Add*.


[[diagnostics]]
=== Diagnostics

From the settings menu, select *Configuration*. Click on the *Diagnostics* tab to see the status of the different {product-title} roles and workers for each server, view and collect logs, and gather data if there are any gaps in capacity and utilization information.
The Diagnostics area is designed in a hierarchy.

* At the *region* level, you can see replication status, backup the VMDB, and run garbage collection on the VMDB.
* At the *zone* level, you can see {product-title} roles by servers and servers by roles. In addition, you can set log collection values for a specific zone, and collect gap data for capacity and utilization.
* At the *server* level, you can see the workers for each server, set log collection values for a specific server, and view current logs.

[[region-diagnostics]]
==== Region Diagnostics

Using the console, you can set the priority of server regional roles, review and reset replication, and create backups of your database either on demand or on a schedule.

Regions are used primarily to consolidate multiple VMDBs into one master VMDB for reporting while zones are used to define functional groups of servers. There can be only one region per VMDB, but multiple zones per region (or VMDB). Some server roles are aware of each other across {product-title} appliances at the region level. This means that redundancy and failover rules apply at the region level. You can also set priorities for the server roles that provide failover.

[[server-role-priorities]]
===== Server Role Priorities

If you have multiple servers in your environment with duplicate failover roles, then you can set the priority of the server role.

* Only server roles that support failover can be marked as primary. These roles only allow one server to be active at a time.
These are *Notifier*, *Capacity & Utilization Coordinator*, *Event Monitor*, *Scheduler*, *Storage Inventory*, and *Provider Inventory*.

* All other server roles are additive. The more servers with that role in a zone the more work that can be performed.

There are three role priorities.

* *Primary*: There can only be one primary per zone or region per role. When an appliance is started, the system looks to see if any role is set to primary.
If that is the case, the role is activated on that appliance and deactivated from the secondary. In the console, primary roles are shown in bold letters.
The text turns red if the server goes down. You must actively set the primary priority.

* *Secondary*: This is the default priority. There can be multiple secondaries. When an appliance is started, if no primary is found in the zone, the first appliance to start takes the role.
In the console, secondary roles are displayed normally with the word "secondary".

* *Tertiary*: If all appliances with primary roles or secondary roles were down, one of the tertiary would be activated.
The reason for tertiary is to ensure that if a server with crucial roles such as Provider Inventory or Event Monitor goes down, you have a way to associate those roles to different appliances by organizing the priorities.
Tertiary roles simply show as active in the console.


[[region-aware-server-roles]]
==== Region Aware Server Roles

[width="100%",cols="50%,25%,25%",options="header",]
|=======================================================================
|Role|More than one per Region|Can have Priority Set
|Automation Engine|Y|N
|Database Operations|Y|N
|Notifier|N|Y
|Reporting|Y|N
|Scheduler|N|Y
|User Interface|Y|N
|Web Services|Y|N
|=======================================================================



[[setting-the-priority-of-a-failover-role]]
==== Setting the Priority of a Failover Role

To set the priority of a failover role:

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Depending on how you want to view your servers, click either the *Roles by Servers* tab or the *Servers by Roles* tab.
. In the *Status* of *Roles for Servers* in *Zone Default Zone* area, click on the role that you want to set the priority for.
. Click image:1847.png[] (*Configuration*), and image:2097.png[] (*Promote Server*) to make this the primary server for this role.
. Click image:1847.png[] (*Configuration*), and image:gui_delete.png[] (*Demote Server*) to demote the priority of this server for this role.

[[zone-diagnostics]]
==== Zone Diagnostics

The console provides a way to see all the server roles that a server has been assigned and if these roles are running. This is especially helpful when you have multiple servers with different server roles. For each zone you can also set a central place for all logs to be collected, and collect capacity and utilization data that may be missing.


[[viewing-the-status-of-server-roles]]
===== Viewing the Status of Server Roles

To view the status of server roles:

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Depending on how you want to view your servers, click either *Roles by Servers* or the *Servers by Roles*.


[[zone-aware-server-roles]]
===== Zone Aware Server Roles

[width="100%",cols="50%,25%,25%",options="header",]
|=======================================================================
|Role|More than one per Region|Can have Priority Set
|Automation Engine|Y|N
|Capacity & Utilization Coordinator|N|Y
|Capacity & Utilization Data Collector|Y|N
|Capacity & Utilization Data Processor|Y|N
|Database Operations|Y|N
|Event Monitor|N|Y
|Provider Inventory|N|Y
|Provider Operations|Y|N
|Notifier|N|Y
|Reporting|Y|N
|Scheduler|N|Y
|SmartProxy|Y|N
|SmartState Analysis|Y|N
|User Interface|Y|N
|Web Services|Y|N
|=======================================================================



[[removing-an-inactive-server]]
====== Removing an Inactive Server

To remove an inactive server:

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Click on the name of the server in the tree view.
. Click image:gui_delete.png[] (*Delete Server*). This button is available only if the server is inactive.

[[zone-log-collections]]
===== Zone Log Collections

If you have multiple servers reporting to one central VMDB, then you can collect the configuration files and logs from the console of any of the servers. While you can set this either at the zone or server level,
settings at the server level supersede the ones at the zone level.

Log depot options include:

* Anonymous File Transfer Protocol (FTP)

* File Transfer Protocol (FTP)

* Network File System (NFS)

* Red Hat Dropbox

* Samba

See your network administrator if you need to set up one of these shares. You
will also need a user that has write access to that location.

[[setting-the-location-of-the-log-depot]]
====== Setting the Location of the Log Depot

To set the location of the log depot:

. From the settings menu, select *Configuration*.
. Click the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Click *Collect Logs*.
. Click image:1851.png[](*Edit*).
. Select the *Type* of share.
image:1851.png[]
. Using the fully qualified domain name (*FQDN*) of the depot server, type in the appropriate settings for the *URI*.
+
image:6254.png[]
+
. If required, enter your user *ID* and *password* then click *Validate* to confirm the settings.
. Click *Save*.

[[collecting-and-downloading-logs-from-all-servers-in-a-zone]]
====== Collecting and Downloading Logs from All Servers in a Zone

To collect and download logs from all servers in a zone:

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Click the *Collect Logs* tab.
. Click image:2104.png[](*Collect all logs*). All files in the logs directory as well as configuration files are collected from the selected zone.
. Click *OK*. The status of the log retrieval shows in the {product-title} console.

[[capacity-and-utilization-repair]]
===== Capacity and Utilization Repair

Under certain circumstances, it is possible that {product-title} is not able to collect capacity and utilization data. This could be due to password expiration, a change in rights to the cloud provider and this change didn't provide enough granularity to the {product-title} service account, or network connectivity. The gap data is collected directly by extracting the monthly performance data. Gap collection need to be completed for each zone individually. Therefore, the procedure below need to be repeated for each zone.

[[repairing-capacity-and-utilization-data]]
====== Repairing Capacity and Utilization Data

To repair capacity and utilization data:

. Log in to a {product-title} appliance located in the zone for which you want to gather the data.
. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Click *C & U Gap Collection*.
.. Select the appropriate *Timezone*.
+
[NOTE]
======
Do not select more than one week unless instructed to do so by Red Hat Support.
======
+
.. Select a *Start Date*.
.. Select an *End Date*.
. Click *Submit*.

After the gap collection has completed for this zone, repeat these same steps for the next zone. You can check for completion by going to the clusters page and checking for the capacity and utilization data for the time period specified.


[[server-diagnostics]]
==== Server Diagnostics

Under *Diagnostics* for a server, you can view the status of {product-title} workers running on the server, set log collection setting for only that server, and view the server's current {product-title} and audit logs.

[[workers]]
===== Workers

The *Workers* tab enables you to see the status of and restart {product-title} workers.

You can see additional information on and restart the following items:

* *C & U Metrics Collectors* that collects capacity and utilization data.
* *C & U Metrics Processors*, which processes the collected capacity and utilization data.
* *Event Handlers* put events from the Event Monitor into the VMDB and starts {product-title} processes if needed base on that information.
* *Event Monitors* that communicate with the external cloud provider to deliver up to date event information.
* *Generic Workers* that perform long running and priority processes.
* *Priority Workers* that perform high priority, short processes.
* *Schedule Workers* that maintains any items that run on a schedule.
* *Session Broker* that maintains a single connection to the cloud providers .
* *Refresh Workers* that runs the refresh processes.
* *Reporting Workers* that generate reports.
* *SmartProxy Workers* that run SmartState Analyses on virtual machine.
* *User Interface Worker* that allows users access to the console.
* *Web Services Worker* that maintains {product-title} Web services.
* *VM Analysis Collectors* that run and process SmartState Analyses on virtual machines.

[[reloading-worker-display]]
====== Reloading Worker Display

To reload worker display:

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Select the server that you want to view.
. Click the *Workers* tab.
. Click image:2106.png[] (*Refresh Current Workers display*).

[[restarting-a-worker]]
====== Restarting a Worker

To restart a worker:

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Select the server that you want to view.
. Click on the *Workers* tab.
. Click on the worker you want to restart.
. Click image:1847.png[] (*Configuration*), then image:2102.png[] (*Restart selected worker*).
. Click *OK*.

[[server-and-audit-logs]]
===== Server and Audit Logs

[[collecting-server-logs-and-configuration-files]]
====== Collecting Server Logs and Configuration Files

While you can designate a central location to collect logs for all
servers in a specific zone, you can override those values for a specific
server. To do this, designate a log depot location to store the files.

Log depot options include:

* Anonymous File Transfer Protocol (FTP)
* File Transfer Protocol (FTP)
* Network File System (NFS)
* Red Hat Dropbox
* Samba

See your network administrator to set up one of these shares. You also
need a user that has write access to that location. Settings at the
server level supersede the ones at the zone level.

[[setting-the-location-of-the-log-depot-for-a-specific-server]]
====== Setting the Location of the Log Depot for a Specific Server

To set the location of the log depot for a specific server

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Select the server that you want to collect logs for.
. Click on the *Collect Logs* tab.
. Click image:1851.png[] (*Edit Log Depot Settings for the selected Server*).
. Select the *Type* of share.
+
image:6254.png[]
+
. Using the fully qualified domain name (*FQDN*) of the depot server, type in the appropriate settings for the *URI*.
. Enter your user ID and password, then click *Validate* to confirm the settings.
. Click *Save*.

[[collecting-the-current-log-set-of-a-server]]
====== Collecting the Current Log Set of a Server

To Collect the Current Log Set of a Server

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Select the server that you want to collect logs for.
. Click on the *Collect Logs* tab.
. Click image:2104.png[] (*Collect*), then click image:2104.png[] (*Collect current logs*). All current log files in as well as configuration files are collected.
. Click *OK*. The status of the log retrieval shows in the {product-title} console.

[[collecting-all-log-sets-from-a-server]]
====== Collecting All Log Sets from a Server

To Collect All Log Sets from a Server

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Select the server that you want to collect logs for.
. Click *Collect Logs*.
. Click image:2104.png[] (*Collect*), then click image:2104.png[] (*Collect all logs*). All files in the logs directory as well as configuration files are collected.
. Click *OK*. The status of the log retrieval shows in the {product-title} console.

[[viewing-the-server,-audit,-and-production-logs]]
====== Viewing the Server, Audit, and Production Logs

The server and audit logs roll over daily. The previous logs are stored as zipped files in the `/var/www/miq/vmdb/log` folder. The current logs can be easily viewed and downloaded from the settings menu; select *Configuration*, then click on the *Diagnostics* accordion.

Use the server log to see all actions taken by the server including communication with the SmartProxy and tasks.

[[viewing-the-server-log]]
====== Viewing the Server Log

To view the server log:

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Select the server that you want to view.
. Click *CFME Log*.

The {product-title} server automatically retrieves the last 1000 lines of the log.

[[reloading-the-server-log]]
====== Reloading the Server Log

To reload the server log:

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Select the server that you want to view.
. Click *CFME Log*.
. Click image:2106.png[] (Reload the Log Display).

[[downloading-the-server-log]]
====== Downloading the Server Log

To download the server log:

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Select the server that you want to view.
. Click *CFME Log*.
. Click image:2107.png[] (*Download the Entire EVM Log File*).

[NOTE]
======
Use the *Audit Log* to see changes to the user interface and authentication.
======

[[viewing-the-audit-log]]
====== Viewing the Audit Log

To view the audit log:

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Select the server that you want to view.
. Click *Audit Log*.

The server automatically retrieves the last 1000 lines of the log.

[[reloading-the-audit-log]]
====== Reloading the Audit Log

To reload the audit log:

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Select the server that you want to view.
. Click *Audit Log*.
. Click image:2106.png[] (*Reload the Audit Log Display*).

[[downloading-the-audit-log]]
====== Downloading the Audit Log

To download the audit log:

. From the settings menu, select *Configuration*.
. Click on the Diagnostics accordion, then click the Zone that you want to view.
. Select the server that you want to view.
. Click Audit Log.
. Click image:2107.png[] (*Download the Entire Audit Log File*).

[[viewing-the-production-log]]
====== Viewing the Production Log

Use the production log to see all actions performed using the console.

To view the production log:

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Select the server that you want to view.
. Click *Production Log*.

The {product-title} server automatically retrieves the last 1000 lines of the log.

[[reloading-the-production-log]]
====== Reloading the Production Log

To reload the production log:

. From the settings menu, select *Configuration*.
. Click on the Diagnostics accordion, then click the Zone that you want to view.
. Select the server that you want to view.
. Click *Production Log*.
. Click image:2106.png[] (*Reload the Product Log Display*).

[[downloading-the-production-log]]
====== Downloading the Production Log

To download the production log:

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click the *Zone* that you want to view.
. Select the server that you want to view.
. Click *Production Log*.
. Click image:2107.png[] (*Download the Production Log File*).


[[database-operations]]
=== Database Operations

[[viewing-information-on-the-vmdb]]
==== Viewing Information on the VMDB

The *Database* accordion displays a summary of VMDB information, a list of all tables and indexes, settings for the tables, active client connection, and database utilization.

To view information on the VMDB:

. From the settings menu, select *Configuration*.
. Click the *Database* accordion.
. Click *VMDB* in the tree view on the left.
. Click the appropriate tab to view the desired information:
* *Summary*: displays statistics about the database.
* *Tables*: displays a clickable list of all the tables.
* *Indexes*: displays a clickable list of all the indexes.
* *Settings*: displays a list of all tables, their descriptions, and other valuable Information.
* *Client Connections*: displays all current connections to the VMDB.
* *Utilization*: displays usage charges for the disk and index nodes.


[[database-regions-and-replication]]
==== Database Regions and Replication

Regions are used to create a central database for reporting and charting. Do not use the database at the top level for operational tasks such as SmartState analysis or capacity and utilization data collection. Assign each server participating in the region a unique number during the regionalization process, then set your subordinate regions to replicate to the top region.

[IMPORTANT]
======
All {product-title} databases in a multi-region deployment must use the same security key.
======

[[creating_a_region]]
===== Creating a Region

In principle, a region is created when you set up your {product-title} environment on the first appliance for the region. However, you can also create a region on an appliance where a database has already been set up. This process involves dropping and rebuilding the existing database to accommodate the new region number, and takes several minutes to complete.

[WARNING]
======
Performing this procedure destroys any existing data and cannot be undone. Back up the existing database before proceeding. By default, new appliances are assigned region 0. Do not use this number when creating a region because duplicate region numbers can compromise the integrity of the data in the database.
======

. Log in to the appliance as the *root* user.
. Enter `appliance_console`, and press *Enter*.
. Press any key.
. Select `Stop EVM Server Processes`.
. Enter `Y` to confirm.
. Choose `Configure Database`.
. Enter a database region number that has not been used in your environment. Do not enter duplicate region numbers because this can corrupt the data.
. Enter `Y` to confirm.
. The menu reappears after all processes are complete.
. Select `Start EVM Server Processes`.
. Enter `Y` to confirm.
. Select *Quit* to exit the advanced settings menu.

[[configuring_database_replication]]
==== Configuring Database Replication and Centralized Administration

To configure database replication, you must configure one {product-title} instance to act as a global copy, and one or more other instances to act as remote copies. This database replication relationship can only be configured on {product-title} instances that are of the same version.

[NOTE]
====
Configuring database replication in this version of {product-title_short} automatically enables centralized administration, eliminating the need for further configuration.
====

Centralized administration in {product-title} supports life cycle management operations that can be initiated from the global copy and processed and executed on the remote copy.

[IMPORTANT]
======
* You must configure at least one remote copy before you can configure the global copy. Changes to the role of a {product-title} instance take several minutes to take effect.

* The region number must be unique on each {product-title} instance where replication is configured. See xref:creating_a_region[] for instructions on how to create a region.
======

[[configuring-a-remote-copy]]
===== Configuring a Remote Copy

Configure a {product-title} instance to act as a remote copy from which data will be replicated to the global copy.

. Navigate to the settings menu.
. Click *Configuration*.
. Click the *Settings* accordion.
. Click the region where the instance is located.
. Click *Replication*.
. Select *Remote* from the *Type* list.
. Click *Save*.

[[configuring-the-global-copy]]
===== Configuring the Global Copy

Configure a {product-title} instance to act as the global copy to which data is replicated from the remote copies.

image:add-subscription-global.png[Add Subscription]

. Log in to the appliance as the *root* user.
. Enter `appliance_console`, and press *Enter*.
. Click *Replication*.
. Select *Global* from the *Type* list.
. Click *Add Subscription*.
.. Enter the name of the database on the remote copy in the *Database* field.
.. Enter the IP address or fully qualified domain name of the remote copy in the *Host* field.
.. Enter the name of a database user able to access the database in the *Username* field.
.. Enter the password of the database user in the *Password* field.
.. Enter the port by which the database is accessed in the *Port* field.
.. In *Actions*, click *Accept*. You can also *Update* your subscription if required, or click the menu on the right in order to *Validate* or *Discard*.
. Click *Save*.

[NOTE]
====
Once you configure a {product-title} instance to act as a global copy, and one or more other instances to act as remote copies, centralized administration is automatically enabled after the initial data sync is complete.
====

Database replication and centralized administration are now enabled on your instances. Centralized administration supports life cycle management operations, including virtual machine power operations and retirement, that can be initiated from the global copy and processed and executed on the remote copy.

ifdef::cfme[]
See the section on _Centralized Administration_ in the https://access.redhat.com/documentation/en-us/red_hat_cloudforms/4.6-beta/html-single/deployment_planning_guide/#central-administration[Deployment Planning Guide] for an overview of the feature.
endif::cfme[]

[[resetting-database-replication]]
===== Resetting Database Replication

You can reset the replication relationship between the global copy and remote copies by temporarily removing and re-enabling the subscription from the global copy.

. From the settings menu, select *Configuration*.
. Click the *Settings* accordion.
. Click the region where the instance is located.
. Click *Replication*.
. Remove the subscription:
.. Click the actions button for the subscription to reset.
.. Click *OK*.
. Click *Save*.
. Re-add the subscription:
.. Click *Add Subscription*.
.. Enter the name of the database on the remote copy in the *Database* field.
.. Enter the IP address or fully qualified domain name of the remote copy in the *Host* field.
.. Enter the name of a database user able to access the database in the *Username* field.
.. Enter the password of the database user in the *Password* field.
.. Enter the port by which the database is accessed in the *Port* field.
. Click *Save*.

//////////////////////////////////////////////
[[running-a-single-backup]]
===== Running a Single Backup

To run a single backup:

. From the settings menu, select *Configuration*.
. Click on the *Diagnostics* accordion, then click Region name.
. Click the *Database* tab.
. If you have created a backup schedule, and want to use the same depot settings, select it under *Backup Schedules*.
. If you do not want to use the settings from a backup schedule, or need to create settings, go into the *Database Backup Settings* area.
. Select a type of server to put the backups. You can either use *Network File System* or *Samba*.
+
image:2099.png[]
+
* If selecting *Samba*, enter the *Depot Name*, *URI*, *User ID*, a *Password*, and a valid *Password*. Click *Validate* to check the settings.
* If you choose *Network File System*, enter the *Depot Name* and *URI*.
. Click *Submit*.

The database backup is run immediately. You can see the task by navigating to menu:Settings[Tasks], then clicking on the *All Other Tasks* tab.

[[restoring-a-database-from-backup]]
===== Restoring a Database from Backup

To restore a database from backup:

. Copy the database backup file to `/tmp`, and name it `evm_db.backup`. The server looks specifically for this file to restore from.
. Log in to the appliance console with a user name of `admin` and the default password. The {product-title} appliance summary screen displays.
. Press `Enter` to manually configure settings.
. Press the number `6` to select *Restore Database From Backup*.
. Press `Y` to confirm.

If directed by Red Hat, you can run database garbage collection to reclaim unused space in your VMDB. Generally, the database server does this automatically.
//////////////////////////////////////////////

[[backing-up-and-restoring-a-database]]
==== Backing Up and Restoring a Database

//////////////////////////////////////////////
[[scheduling-a-database-backup]]
===== Scheduling a Database Backup

Schedule database backups on a regular basis to preserve data.

To schedule a database backup:

. From the settings menu, select *Configuration*.
. Click the *Settings* accordion and click *Schedules*.
. Click image:1847.png[] (*Configuration*), and image:plus_green.png[] (*Add a new Schedule*).
. Under the *Adding a new Schedule*:
+
image:2108.png[]
+
.. Enter a *Name* and *Description* for the schedule.
.. Check *Active* to enable the backup schedule.
.. In the *Action* drop-down list, select *Database Backup*.
.. Select a type of server for storing the backups from the *Type* drop-down list. You can use *Network File System* (NFS) or *Samba*.
* If you select *Samba*, enter the *Depot Name*, *URI*, *User ID*, and a valid *Password*. Click *Validate* to check the settings.
* If you select *Network File System*, enter the *Depot Name*, and *URI*.
+
The *Depot Name* is a human-readable label for the NFS or Samba share.
. Select the desired backup frequency from the *Run* list:
+
image:2110.png[]
+
* *Once*: the backup runs one time.
* *Hourly*: select the number of hours between backups from the drop-down list.
* *Daily*: select the number of days between backups from the drop-down list.
* *Weekly*: select the number of weeks between backups from the drop-down list.
* *Monthly*: select the number of months between backups from the drop-down list.
. Select a *Time Zone* for the schedule.
. Type or select a *Starting Date* for the schedule.
. Select a *Starting Time* based on a 24 hour clock.
. Click *Add*.
//////////////////////////////////////////////

[[running-a-single-database-backup]]
===== Running a Single Database Backup

To run a single database backup:

. From the settings menu, select *Configuration*.
. Click the *Diagnostics* accordion and click the *Region* name.
. Click the *Database* tab.
. If you have created a backup schedule and want to use the same depot settings, select the schedule in the *Backup Schedules* box.
. If you do not want to use the settings from a backup schedule, select a type of server for storing the backups from the Type drop-down list in the *Database Backup Settings* box. You can use *Network File System* (NFS) or *Samba*.
* If you select *Samba*, enter the *URI*, *User ID*, and a valid *Password*. Click *Validate* to check the settings.
* If you select *Network File System*, enter the *URI*.
. Click *Submit* to run the database backup.

[[restoring-a-database-from-a-backup]]
===== Restoring a Database from a Backup

If a database is corrupt or fails, restore it from a backup. You can restore a backup from a local file, NFS, or Samba.

To restore a database from a backup:

. Save the database backup file as `/tmp/evm_db.backup`. {product-title} looks specifically for this file when restoring a database from a local backup.
. If you are restoring a database backup on a high availability environment, stop the `regmgrd` service. This is not required in other {product-title_short} configurations.
+
----
# systemctl stop rh-postgresql95-repmgr
----
+
. Log in to the appliance as the *root* user.
. Enter `appliance_console`, and press *Enter*.
. Select `Stop EVM Server Processes` to stop processes on all servers that connect to this VMDB.
. Enter `Y` to confirm.
. After all processes are stopped, press *Enter* to return to the menu.
. Press *Enter* again to manually configure settings.
. Select `Restore Database From Backup`, then specify the location to restore the backup from in the `Restore Database File` menu:
.. If you saved the database backup file locally as `/tmp/evm_db.backup`, select `Local file`. You can also restore from a `Network File System (nfs)` or `Samba (smb)`.
.. Specify the location of the backup file.
+
[NOTE]
====
The appliance console menu may respond slowly if connections are open and the server is still shutting down. If this occurs, wait a minute and try again.
====
+
. Enter `Y` to keep the database backup after restoring from it. Enter `N` to delete it.
. Press `Y` to confirm.
. After the backup completes, press *Enter* to return to the menu.
. Press *Enter* again to manually configure settings.
. Select `Start EVM Server Processes` to restart all processes on servers that connect to this VMDB.
. Enter `Y` to confirm.
. If you are restoring a database backup on a high availability environment, start the `regmgrd` service. This is not required in other {product-title_short} configurations.
+
----
# systemctl start rh-postgresql95-repmgr
----

[[binary-backup-and-restore-database]]
==== Performing a Binary Backup and Restoring the Database
Preserve data at the file system level by performing a binary backup.
This includes all databases, users and roles, and other objects.

[NOTE]
====
This procedure uses the `pg_basebackup` utility to perform a remote database backup and create a full replacement of the PostgreSQL data directory, capturing the exact state of the database when the backup finishes. For more information on the `pg_basebackup` utility, see the PostgreSQL documentation.
====

===== Performing a Binary Backup

Create a binary backup and store it as a `gzip` tar file inside the {product-title_short} backup directory.

[IMPORTANT]
======
PostgreSQL superuser or user with _Replication_ permissions are required to perform this procedure.
======

. SSH into the database server as the *root* user or provide PostgreSQL superuser credentials.
+
. Run the `pg_basebackup` command to create the backup.
+
-----
# pg_basebackup -x -h hostname -U root -Ft -z -D filename
-----
+
> where:
>
> *-h* _hostname_
>
>     Specifies the IP address of the database server.
>
> *-D* _filename_
>
>    Specifies the name of the directory created to contain the backup.


===== Restoring a Database from the Backup
Restore your PostgreSQL binary backup using the following steps. This process will require stopping both EVM and PostgreSQL services before restoring data.

. Copy the existing backup to the target VM
+
-----
# scp filename/base.tar.gz root@hostname:/var/www/miq
-----
+
. SSH to the target VM.no

+
-----
# ssh root@hostname
-----
+
. Stop both the EVM and PostgreSQL servers.
+
-----
# systemctl stop evmserverd
# systemctl stop $APPLIANCE_PG_SERVICE
-----
+
. Rename the existing data directory.
+
-----
# mv /var/opt/rh/rh-postgresql95/lib/pgsql/data /var/opt/rh/rh-postgresql95/lib/pgsql/data.backup
-----
+
. Create a clean data directory.
+
-----
# mkdir /var/opt/rh/rh-postgresql95/lib/pgsql/data
-----
+
. Unzip the tar file to the new directory.
+
-----
# tar -xzf /var/www/miq/base.tar.gz -C /var/opt/rh/rh-postgresql95/lib/pgsql/data
-----
+
. Correct permissions.
+
-----
# chown postgres:postgres /var/opt/rh/rh-postgresql95/lib/pgsql/data
# chmod 700 /var/opt/rh/rh-postgresql95/lib/pgsql/data
-----
+
. Restart the PostgreSQL and EVM servers.
+
-----
# systemctl start $APPLIANCE_PG_SERVICE
# systemctl start evmserverd
-----

[[running-database-garbage-collection]]
==== Running Database Garbage Collection

The database server collects garbage automatically, but Red Hat may occasionally direct you to run database garbage collection manually in order to reclaim unused space in your VMDB.

To run database garbage collection:

. From the settings menu, select *Configuration*.
. Click the *Diagnostics* accordion and click the *Region* name.
. Click the *Database* tab.
. In the *Run Database Garbage Collection Now* box, click *Submit*.


[[changing-database-password]]
==== Changing Database Password

[[changing-the-password-on-the-database-appliance]]
===== Changing the Password on the Database Appliance

{product-title} provides a default database password for the internal PostgreSQL database.

To change the password, you need to stop the {product-title} Service, change the password for the PostgreSQL database, run a command to change the password in the configuration file that `evmserverd` uses to access the server, and restart the {product-title} appliance.

. Stop the {product-title} service:
.. SSH into the appliance.
.. To stop the {product-title} service, run the following command:
+
------
service evmserverd stop
------
+
. Use `pgadmin` to change the password for your {product-title} database (default is `vmdb_production`). If you do not have `pgadmin`, you can change the password by running:
+
------
psql -U root -d vmdb_production
------
+
.. At the vmdb# prompt, type:
+
------
ALTER USER root WITH PASSWORD 'newpassword';
------
+
.. To exit psql, type:
+
------
\q
------
+
. Change the password in the configuration file that `evmserverd` uses to access the server:
+
------
/var/www/miq/vmdb/tools/fix_auth.rb --databaseyml --password newpassword
------
+
. Restart the {product-title} service:
+
------
service evmserverd start
------
+
. Verify that you can log in to the {product-title} console.


[[changing-the-password-on-the-worker-appliances]]
===== Changing the Password on the Worker Appliances

. Stop the {product-title} service:
.. SSH into the appliance.
.. To stop the {product-title} service, run the following command:
+
------
service evmserverd stop
------
+
. Change the password in the configuration file that `evmserverd` uses to access the server:
+
------
/var/www/miq/vmdb/tools/fix_auth.rb --databaseyml --password newpassword
------
+
. Restart the {product-title} service:
+
------
service evmserverd start
------
+

[IMPORTANT]
====
*Requisites for Replication*

If you change the password on the remote database, you will need to change the password in the subscription.
====

[[adding-a-new-appliance-to-an-existing-region]]
==== Adding a New Appliance to an Existing Region with a Non-default Password

. Create the new appliance.
. Start the appliance, but do not go into any of the configuration options, instead *SSH* into the new appliance.
. In the */var/www/miq/vmdb* directory, create a file called `REGION`. Its only contents should be the number of the Region that it is joining. (You could also just copy the `REGION` file from the VMDB appliance.)
. Edit the *database.yml* file in the */var/www/miq/vmdb* directory. (You may want to save from the original.)
.. Replace the contents of the *"production"* section with the contents of the *"base"* section.
.. Edit the *"host"* parameter to match the IP of the appliance hosting the VMDB.
.. Save the new *database.yml*.
. Run the following command to change the password in the configuration file that `evmserverd` uses to access the server:
+
------
/var/www/miq/vmdb/tools/fix_auth.rb --databaseyml --password newpassword
------
+
. Restart the new worker appliance:
+
------
service evmserverd restart
------
+


[[configuring-database-maintenance]]
==== Configuring Scheduled Database Maintenance

You can schedule hourly or periodic database maintenance through the appliance console. Performing regular PostgreSQL database maintenance helps to maintain a more responsive {product-title} environment.

Hourly database maintenance tasks, such as reindexing, are useful for highly active database tables such as metrics, workers, and servers.

You also may want to perform periodic database maintenance to truncate empty metrics tables and reorganize the database. Periodic maintenance can be configured to run hourly, daily, weekly, or monthly, at a specified hour and on a specified day.

[NOTE]
====
Periodic maintenance can impact appliance performance while it is running. Red Hat recommends scheduling periodic maintenance infrequently, and at off hours.
====

To configure hourly and periodic database maintenance:

. Log in to the appliance as the *root* user.
. Enter `appliance_console`, and press *Enter*.
. Press any key.
. Select *Configure Database Maintenance* to configure the automatic database maintenance schedule through a dialog.
.. For *Configure Hourly Database Maintenance?* Type `y` or `n`.
.. For *Configure Periodic Database Maintenance?* Type `y` or `n`.

The next options depend on the periodic database maintenance frequency you choose, and are specified using the same dialog. The dialog finishes configuration with a *"Database maintenance configuration updated"* message when complete.

To reset your database maintenance settings, enter *Configure Database Maintenance* again from the appliance console menu, and confirm that you want to unconfigure the settings in the configuration dialog. This deletes the current settings.

To configure a new database maintenance schedule, enter the *Configure Database Maintenance* menu item once again and configure the values using the dialog.
