[[installing-cloudforms]]
== Installing {product-title}

{product-title} can be installed on OpenShift Container Platform in a few steps. 

This procedure uses a template to deploy a multi-pod {product-title_short} appliance with the database stored in a persistent volume on OpenShift Container Platform. It provides a step-by-step setup, including cluster administrative tasks as well as information and commands for the application developer using the deployment. 

The ultimate goal of the deployment is to be able to deconstruct the {product-title_short} appliance into several containers running on a pod or a series of pods. 

Running the {product-title_short} appliance in a series of pods has several advantages. For example, running each worker in a separate pod allows OpenShift Container Platform to manage worker processes and reduce worker memory consumption. OpenShift can also easily scale workers by adding or removing pods, and perform upgrades by using images. (OpenShift also provides built-in health Check like probing HTTP which can be used to understand health and alerts??)

[[prerequisites]]
=== Prerequisites

To successfully deploy a {product-title_short} appliance on OpenShift Container Platform, you need a functioning *OpenShift Container Platform 3.7* install with the following configured:

* NFS or other compatible volume provider
* A `cluster-admin` user
* A regular user (such as an application developer)

[IMPORTANT]
====
OpenShift Container Platform 3.7 is required for this installation. Red Hat has not tested this procedure with earlier versions of OpenShift Container Platform.
====

The {product-title_short} deployment uses three files to create the appliance: `cfme-template.yaml`, which is the {product-title_short} template used for the deployment, and `cfme-pv-example.yaml` and `cfme-pv-app-example.yaml`, two pod volume files. 

_These template files, and others, are available in OpenShift 3.7. They can also be found in GitHub (supported?):_ https://github.com/openshift/openshift-ansible/tree/master/roles/openshift_examples/files/examples/v3.7/cfme-templates

These files create the OpenShift pods comprising the appliance, and are available from the https://access.redhat.com/containers/?product=Red%20Hat%20CloudForms%20#/search/cloudforms[Red Hat Container Catalog]:


Pods for (each is a pod):

// Check these.
* {product-title_short} appliance image
* Memcached image
* PostgreSQL image
* Embedded Ansible image 
* Apache image

To download the files to your system, navigate to each container image page, and click the *Get this image* tab. Select *Red Hat OpenShift* from the *Choose your platform:* list, then follow the instructions on that page to pull the image to your system.

//Download them first? Refer to another section for instructions?

/////
OpenShift Container Platform 3.5 includes these files by default.
/////

==== Cluster Sizing

To avoid deployment failures due to resource starvation, Red Hat recommends the following minimum cluster size for a test environment:

* 1 master node with at least 8 vCPUs and 12GB RAM
* 2 schedulable nodes with at least 4 vCPUs and 8GB RAM
* 25GB storage for {product-title_short} physical volume use

These recommendations assume {product-title_short} is the only application running on this cluster. Alternatively, you can provision an infrastructure node to run registry, metrics, router, and logging pods.

Each {product-title_short} application pod will consume at least 3GB RAM on initial deployment (without providers added). RAM consumption increases depending on the appliance use. For example, after adding providers, expect higher resource consumption.

==== Limitations

The following limitations exist when deploying this version of {product-title_short} on OpenShift Container Platform 3.7:

* This configuration cannot run on public OpenShift (OpenShift.io and OpenShift Dedicated environments) because of necessary privileges
* The Embedded Ansible pod must run as a privileged pod
* OpenShift cannot independently scale workers - can't just scale a particular role. 
* A highly available database is not supported in Postgres pods
* Httpd + cfme-app set both do some LB (rewrite and explain) 15:13 - there is some overlap between pods doing load balancing. (still don't get it)


External auth 
- all external authentication configurations supported by CloudForms are supported in this config. (happens in the Apache pod)
IPA - user/password, Kerberos SSO. OTYP
Active Directory - user/password, kerberos  SSO
LDAP - user/password
SAML/Keycloak


17:49
2 options. 
  
- When you install OCP 3.7, you have the option to install CloudForms inside OPenshift at the time. Leverages the Ansible installer to run and deploy the CloudForms template. (Ansible does it instead oif manual building)

  * find out - is this documented in the OpenShift installation guide? Maybe they should point to this guide for if they don't want to install CF at installation time.

- Install manually using the CloudForms openshift template (yaml files)


Multi-pod appliance
DB stored in PV
statefulset for server appliance also stored in a persistent volume
Requirements 
OpenShift 3.7 
NFS or other volume provider(do not use NFS for Database) - NFS mount
Cluster-admin access - for security contexts

recommendations
1 x Master node - 8 vCPU, 12GB memory
2 x schedulable nodes - 4 vCPU, 8GB memory
20GB bare minimum storage for CFME PVs




[[preparing-for-deployment]]
=== Preparing to Deploy {product-title_short}

To prepare for deploying the {product-title_short} appliance to OpenShift Container Platform, create a project, configure security contexts, and create pod volumes.

. As a regular user, log in to OpenShift: 
+
----
$ oc login -u <user> -p <password>
----
+
. Create a project with your desired parameters. The project name (`<your_project>` in this example) is mandatory, but `<description>` and `<display_name>` are optional: 
+
----
$ oc new-project <your_project> \
--description="<description>" \
--display-name="<display_name>"
----
+
. As the admin user, configure security context constraints (SCCs) for your OpenShift service accounts. The following table provides details on the SCCs:
+
.Required Security Context Constraints (SCCs)
[cols="1,1,1", frame="all", options="header"]
|====
|  Service Account|  Security Context Constraint (SCC)|  Description
|  `cfme-anyuid`|`anyuid`|Allows pods using `cfme-anyuid` to run as root because the {product-title_short} image requires the root user.
‎| `cfme-orchestrator`|`anyuid`|DESCRIPTION NEEDED
| `cfme-httpd`| `anyuid`| DESCRIPTION NEEDED
|`cfme-privileged` | `privileged`| DESCRIPTION NEEDED
|====
+
To configure the SCCs, run the following commands as the admin user:
+
.. Add the `cfme-anyuid` service account to the `anyuid` SCC:
+
----
$ oc adm policy add-scc-to-user anyuid system:serviceaccount:<your-project>:cfme-anyuid
----
+
.. Add the `cfme-orchestrator` service account to the `anyuid` SCC:
+
----
$ oc adm policy add-scc-to-user anyuid system:serviceaccount:<your-project>:cfme-orchestrator
----
+
.. Add the `cfme-httpd` service account to the `anyuid` SCC:
+
----
$ oc adm policy add-scc-to-user anyuid system:serviceaccount:<your-project>:cfme-httpd
----
+ 
.. Add the `cfme-privileged` service account to the `privileged` SCC:
+
----
$ oc adm policy add-scc-to-user privileged system:serviceaccount:<your-project>:cfme-privileged
----
+
. Verify the SCCs are added correctly to the service accounts and project:
+
----
$ oc describe scc anyuid | grep Users
Users:					system:serviceaccount:<your-project>:cfme-anyuid,system:serviceaccount:<your-project>:cfme-httpd,system:serviceaccount:<your-project>:cfme-orchestrator
$ oc describe scc privileged | grep Users
Users:					system:admin,system:serviceaccount:openshift-infra:build-controller,system:serviceaccount:management-infra:management-admin,system:serviceaccount:management-infra:inspector-admin,system:serviceaccount:logging:aggregated-logging-fluentd,system:serviceaccount:<your-project>:cfme-privileged
----
+
. Add the `view` and `edit` roles to the `cfme-orchestrator` service account:
+
----
$ oc policy add-role-to-user view system:serviceaccount:<your-project>:cfme-orchestrator -n <your-project>
$ oc policy add-role-to-user edit system:serviceaccount:<your-project>:cfme-orchestrator -n <your-project>
----
+

//TODO: Add more verification commands above.

////

MOVE VERIFY STEPS DOWN
.. Verify that the `cfme-anyuid` service account is now included in the `anyuid` SCC:
+
------
$ oc describe scc anyuid | grep Users
Users:					system:serviceaccount:<your-project>:cfme-anyuid
------
+
. Add your default service account to the `privileged` security context. The default service account for your project (project) must be added to the `privileged` security context constraints (SCCs) before they can run privileged pods.
+
.. As the admin user, add the default service account by running:
+
------
$ oc adm policy add-scc-to-user privileged system:serviceaccount:<your-project>:default
------
+
.. Verify that your default service account is now included in the `privileged` security context constraints (SCCs):
+
------
$ oc describe scc privileged | grep Users
Users:                  system:serviceaccount:openshift-infra:build-controller,system:serviceaccount:management-infra:management-admin,system:serviceaccount:management-infra:inspector-admin,system:serviceaccount:default:router,system:serviceaccount:default:registry,system:serviceaccount:<your-project>:default
------
+
////
. Prepare persistent storage for the deployment. (Skip this step if you have already configured persistent storage.) 
+
A basic {product-title_short} deployment needs at least two persistent volumes (PVs) to store {product-title_short} data. As the admin user, create two persistent volumes: one to host the {product-title_short} PostgreSQL database, and one to host the application data. 
+
Example NFS-backed volume templates are provided by `cfme-pv-db-example.yaml` and `cfme-pv-server-example.yaml`, available from https://github.com/openshift/openshift-ansible/tree/master/roles/openshift_examples/files/examples/v1.5/cfme-templates/[GitHub]. 
+
[NOTE]
====
For NFS-backed volumes, ensure your NFS server firewall is configured to allow traffic on port 2049 (TCP) from the OpenShift cluster.

Red Hat recommends setting permissions for the pv-app (privileged pod volume) as 777, uid/gid 0 (owned by root). For more information on configuring persistent storage in OpenShift Container Platform, see the https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/installation_and_configuration/#configuring-persistent-storage[OpenShift Container Platform Installation and Configuration] guide.	
====
+
.. Configure your NFS server host details within these files, and edit any other settings needed to match your environment.
+
.. Run the following commands to create the two persistent volumes: 
+
------
$ oc create -f cfme-pv-db-example.yaml
$ oc create -f cfme-pv-server-example.yaml
------
+
.. Verify the pod volumes were created successfully: 
+
------
$ oc get pv
NAME       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM  REASON   AGE
cfme-pv01   15Gi        RWO           Recycle         Available                   30s
cfme-pv02   5Gi         RWO           Recycle         Available                   19s
------
+
[NOTE]
====
Red Hat recommends validating NFS share connectivity from an OpenShift node before attempting a deployment.
====
+
. Increase the maximum number of imported images on ImageStream.
+
By default, OpenShift Container Platform can import five tags per image stream, but the {product-title_short} repositories contain more than five images for deployments.
+
You can modify this setting on the master node at `/etc/origin/master/master-config.yaml` so OpenShift can import additional images. 
+
.. Add the following at the end of the `/etc/origin/master/master-config.yaml` file: 
+
----
...
imagePolicyConfig:
  maxImagesBulkImportedPerRepository: 100
----
+
.. Restart the master service:
+
----
$ systemctl restart atomic-openshift-master
----



[[deploying-the-appliance]]
=== Deploying the {product-title_short} Appliance

To deploy the appliance on OpenShift Container Platform, create the {product-title_short} template and verify it is available in your project. 

. As a regular user, create the {product-title_short} template: 
+
------
$ oc create -f cfme-template.yaml
template "cloudforms" created
------
+
. Verify the template is available with your project: 
+
------
$ oc get templates
NAME         DESCRIPTION                                    PARAMETERS        OBJECTS
cloudforms   CloudForms appliance with persistent storage   18 (1 blank)      12
------
+
. (Optional) Customize the template’s deployment parameters. Use the following command to see the available parameters and descriptions:
+
------
$ oc process --parameters -n <your-project> cloudforms
------
+
To customize the deployment configuration parameters, run:
+
------
$ oc edit dc/<deployconfig_name>
------
+
. To deploy {product-title_short} from template using default settings, run: 
+
------
$ oc new-app --template=cloudforms
------
+
Alternatively, to deploy {product-title_short} from a template using customized settings, add the `-p` option and the desired parameters to the command. For example: 
+
------
$ oc new-app --template=cloudforms -p DATABASE_VOLUME_CAPACITY=2Gi,MEMORY_POSTGRESQL_LIMIT=4Gi,APPLICATION_DOMAIN=hostname
------
+
[IMPORTANT]
====
The `APPLICATION_DOMAIN` parameter specifies the hostname used to reach the {product-title_short} application, which eventually constructs the route to the {product-title_short} pod. If you do not specify the `APPLICATION_DOMAIN` parameter, the {product-title_short} application will not be accessible after the deployment; however, this can be fixed by changing the route. For more information on OpenShift template parameters, see the https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-templates[OpenShift Container Platform Developer Guide].
====

[[deploying-the-appliance-external-db]]
==== Deploying the {product-title_short} Appliance Using an External Database

Before attempting to deploy {product-title_short} using an external database deployment, ensure the following conditions are satisfied:

* Your OpenShift cluster can access the external PostgreSQL server
* The {product-title_short} user, password, and role have been created on the external PostgreSQL server
* The intended {product-title_short} database is created, and ownership has been assigned to the {product-title_short} user

To deploy the appliance:

. Import the {product-title_short} external database template:
+
----
$ oc create -f templates/cfme-template-ext-db.yaml
----
+
. Launch the deployment with the following command. The database server IP address is required, and the other settings must match your remote PostgreSQL server.
+
----
$ oc new-app --template=cloudforms-ext-db -p DATABASE_IP=<server_ip> -p DATABASE_USER=<user> -p DATABASE_PASSWORD=<password> -p DATABASE_NAME=<database_name>
----

[[verifying-the-configuration]]
=== Verifying the Configuration

Verify the deployment was successful by running the following commands as a regular user under the {product-title_short} project:

[NOTE]
====
The first deployment can take several minutes to complete while OpenShift downloads the necessary images. 
====

. Confirm the {product-title_short} pod is bound to the correct security context constraints. 
.. List and obtain the name of the `cfme-app` pod: 
+
------
$ oc get pod
NAME                 READY     STATUS    RESTARTS   AGE
cloudforms-1-fzwzm   1/1       Running   0          4m
memcached-1-6iuxu    1/1       Running   0          4m
postgresql-1-2kxc3   1/1       Running   0          4m
------
+
.. Export the configuration of the pod: 
+
------
$ oc export pod <cfme_pod_name>
------
+
.. Examine the output to verify that `openshift.io/scc` has the value `anyuid`: 
+
------
...
metadata:
  annotations:
    openshift.io/scc: anyuid
...
------
+
. Verify the persistent volumes are attached to the `postgresql` and `cfme-app` pods:
+
------
$ oc volume pods --all
pods/postgresql-1-437jg
  pvc/cfme-pgdb-claim (allocated 2GiB) as cfme-pgdb-volume
    mounted at /var/lib/pgsql/data
  secret/default-token-2se06 as default-token-2se06
    mounted at /var/run/secrets/kubernetes.io/serviceaccount
pods/cfme-1-s3bnp
  pvc/cfme (allocated 2GiB) as cfme-app-volume
    mounted at /persistent
  secret/default-token-9q4ge as default-token-9q4ge
    mounted at /var/run/secrets/kubernetes.io/serviceaccount
------
+
. Check the readiness of the {product-title_short} pod: 
+
[NOTE]
====
Allow approximately five minutes once pods are in running state for {product-title_short} to start responding on HTTPS.  
====
+
----
$ oc describe pods <cfme_pod_name>
...
Conditions:
  Type      Status
  Ready     True
Volumes:
...
----
+
. After you have successfully validated your {product-title_short} deployment, disable automatic image change triggers to prevent unintended upgrades.
+
By default, on initial deployments the automatic image change trigger is enabled. This could potentially start an unintended upgrade on a deployment if a newer image is found in the ImageStream.
+
Disable the automatic image change triggers for {product-title_short} deployment configurations (DCs) on each project with the following commands:
+
----
$ oc set triggers dc --manual -l app=cloudforms
deploymentconfig "memcached" updated
deploymentconfig "postgresql" updated

$ oc set triggers dc --from-config --auto -l app=cloudforms
deploymentconfig "memcached" updated
deploymentconfig "postgresql" updated
----
+
[NOTE]
====
The configuration change trigger is kept enabled; to have full control of your deployments, you can alternatively turn it off. See the https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-triggering-builds[OpenShift Container Platform Developer Guide] for more information on deployment triggers.
====

=== Logging into {product-title_short}

Once the pods have been successfully deployed, you can log into {product-title_short} and perform administration tasks. 

You can obtain host information to log into the appliance from the project in the OpenShift user interface, or by opening a shell on the pod and getting the route information:

. To open a shell on the pod, run:
----
$ oc rsh <pod_name> bash -l
----
+
. Get the route information
----
$ oc get routes
NAME         HOST/PORT                   PATH                SERVICE      TERMINATION   LABELS
cloudforms   cfme.apps.e2e.example.com  cloudforms:443-tcp   passthrough                app=cloudforms
----

A route should have been deployed via template for HTTPS access on the {product-title_short} pod. Examine the output and point your web browser to the reported URL/host (in this example, `cfme.apps.e2e.example.com`).

.  Navigate to the URL for the login screen. (https://xx.xx.xx.xx on the virtual machine instance)
.  Enter the default credentials (Username: *admin* | Password: *smartvm*) for the initial login.
.  Click *Login*.

////

[[pod-access-and-routes]]
==== Obtaining Pod Access and Routes
is this the same as finding out how to log into the new cloudforms appliance? 
//// 




